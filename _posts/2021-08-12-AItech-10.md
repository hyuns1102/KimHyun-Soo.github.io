---
title: "[네이버 부스트 캠프] AI-Tech-re - week2(4) - Transformer"
categories: AI boostcourse
tags: python
published: true
use_math: true
---

#### Transformer

[원본 링크](https://nlpinkorean.github.io/illustrated-transformer/)를 참고해서 보자.

단어가 끊기거나 생략되거나 한다면 ? 어떻게 해야할까? 라는 의문에서 시작되었다.  

인코딩 이전에 linear 모델을 통과한 후에 나온 vector를 이용해서 인코딩을 시작한다.  

- Sequence to Sequence Transformer
  Sequence 하나당 n개의 단어 모두를 인코딩할 수 있다.  

    a. N개의 단어가 어떻게 인코더에서 한방에 처리?
    b. 인코더와 디코더 사이에 어떤 정보 주고 받음?
    c. 디코더가 어떻게 generalize 할 수 있는지?

  - a. N개의 단어를 어떻게 인코더에서 처리할까?

    Feed forward ~ 는 MLP랑 동일한 구조라고 생각하자.

    ![s1](/assets/images/AI-Images/img49.png)

    - Self-attention : 3개의 단어에 대한 3개의 벡터 -> 3개의 벡터를 출력해준다.  
      여기서 self-attention은 나머지 단어들에 대해 dependencies가 존재한다.  
      Feed forward에 대해선 그대로 출력한다.  
      ex) self-attention at high-level  

    - Encoding 과정  
      Embedding vector / Queries / Keys / Values 의 벡터들을 하나씩 생성해주고  
      Queries, Keys vector사이의 내적을 통해 Score vector를 만들어준다. (얼마나 관계 있는지)  

      ![s2](/assets/images/AI-Images/img50.png)

      그 다음 Score를 Normalization 한 후, Softmax 함수를 통해 계산한 다음에 가지고 있는 Value vector에 곱해준다. (weighted Sum of the value vectors)  
      Query : 디코더의 이전 레이어 hidden state  
      K : 인코더의 output state  
      V : 인코더의 output state  

      ![s3](/assets/images/AI-Images/img51.png)

      이 과정을 통해 z1 vector를 만들어낸다.  
      Key, Query, Value vector 각각 찾아내는 MLP가 존재한다. 위의 과정을 간단하게 나타내면  

      ![s4](/assets/images/AI-Images/img52.png)

      이 구조는 입력과 출력이 고정되어 있는 구조가 아니라, 입력에 어떤 다른 Sequence가 들어오면 출력이 계속 달라지게 된다.  
      즉, 표현력이 많아진다. -> 많은 computation(계산) 이 필요  

    - Multi Headed Attention (MHA)  
      하나의 Input에 N개의 attention을 반복하게 되면 MHA라고 한다.

      ![s5](/assets/images/AI-Images/img53.png)

      이러한 MHA를 통해 나온 Z 벡터들은 concatenating을 하고 W0라는 큰 가중치 벡터에 의해 output Z 생성

    - Positional Encoding  
      Sequence는 어떻게 들어오든 값이 encoding에 의해 달라질 수가 없다. (order에 independent)
      어떤 단어가 어느 위치에 있는 지 중요하기에 넣어준다. (벡터값에 offset을 줌)

      ![s6](/assets/images/AI-Images/img54.png)

  - b. 인코더와 디코더 사이에 어떤 정보 주고 받음?

      ![s7](/assets/images/AI-Images/img55.png)

      Encoder에서 Decoder로 갈 때, 어떤 정보가 전달되어지는가?

      ![s8](/assets/images/AI-Images/img56.png)
      ![s9](/assets/images/AI-Images/img57.png)

      Key Code 와 Value vector 값을 Decoder에 전달해서 previous output과 함께 output이 나오게 된다.

    - Self-attention layer & Encoder-Decoder Attention  
      Masking :  이전 단어들에만 dependent 하도록 하고,.. 무슨 뜻인지 이해 불가..  

      ![s10](/assets/images/AI-Images/img58.png)
      ![s11](/assets/images/AI-Images/img59.png)

실습 - Vit