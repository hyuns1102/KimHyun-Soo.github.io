---
title: "[네이버 부스트 캠프] AI-Tech-re - week1(5)"
categories: AI boostcourse
tags: python
published: true
use_math: true
---

## 학습기록 - 5

오늘 한 일

- AI - math : CNN, RNN
- 선택과제 2, 3번 -> 못품. (질문 및 정답 확인, 리뷰)
- numpy, pandas

### 1. 강의 복습 내용

- CNN : Convolutional Neural Network
  CNN의 경우 가장 기본적으로 Convolution 연산을 기초로 한다.  
  ![s1](/assets/images/AI-Images/img4.PNG)

  모델은 커널과 입력 값을 통해 출력이 결정된다.  

  ![s2](/assets/images/AI-Images/img5.PNG)

  그리고 커널은 변하지 않고 입력 값의 위치를 이동하며 출력값을 결정한다.  

  ![s3](/assets/images/AI-Images/img6.PNG)

  그렇기에 출력크기는 입력크기를 (H,W), 커널 크기를 (K_H, K_W), 출력 크기를 (OH, OW)라 할 때, 다음과 같이 나타낼 수 있다.  

  ![s4](/assets/images/AI-Images/img7.PNG)

  Back Propagation 또한, 출력할 때의 연산과 똑같은 방식으로 하면, 식이 비슷해진다.  

  ![s5](/assets/images/AI-Images/img8.PNG)

- RNN : Recurrent Neural Network
  RNN은 이전의 연속적인 데이터로 이후 데이터를 예측하는 모델이다. 주로 언어, 주식 등에 쓰인다.  
  RNN의 경우, 시퀀스 데이터를 다루기 때문에 과거의 자료들을 쓰게된다.  
  이 때, 바로 이전 자료를 제외한 **나머지 자료**를 H(x)라는 잠재변수로 인코딩해서 신경망을 만든다.

  ![s6](/assets/images/AI-Images/img9.PNG)

  그렇기에 신경망의 식 또한, 잠재벡터와 함께 이전 잠재벡터, 이전 가중치의 정보를 함께 넣게 된다.  
  역전파 또한 같은 방법으로 넣어지게 된다. 이를 Back Propagation Through Time (BPTT) 라 한다.

  ![s7](/assets/images/AI-Images/img10.PNG)

  BPTT의 경우 가장 중요한 포인트는 Vanishing Gradient 부분이다. 이것은 시퀀스를 다루는 RNN에서 생각해야할 문제인데,  
  잠재 벡터의 가중치에 대해서 그래디언트를 구하게 될 때, 해당 시퀀스의 길이가 길어질 수록 불안정해지는 현상이다.  
  물론 많이 짧다면, 적절한 BPTT를 할 수 없다. 따라서 이 길이를 적절하게 해결하기 위한 알고리즘 모델이 **LSTM, GRU** 이다.

  ![s8](/assets/images/AI-Images/img11.PNG)



### 2. 고민 내용, 결과 (과제 수행 과정, 결과물 정리)

1. 선택 과제 1번 - SGD.

Error에 대해서 구할 때 L2 norm을 사용해서 구하려 했으나 오류가 뜨게 된다.  
왜 그런지 이유를 아직 파악하지 못했다. 질문이 필요한 것 같다.  

2. 선택 과제 2번 - RNN.

RNN의 BPTT 에서 Wrec 과 Wx 의 Gradient를 구하는 문제였다.  
문제를 이해하기 전에, 해당 이론의 편미분을 구하려 했으나 제대로 되지 않았다.  
그래서 파이세션 질문을 통해, 과정에 대해 알게 되었다.  

### 3. 피어세션 정리

1. 강의 요약 발표 진행
    - 내용: CNN, RNN
    - 베이즈 정리 관련 질문 :

    $ P\left(X_{1}, \ldots, X_{t}\right)=P\left(X_{t} \mid X_{1}, \ldots, X_{t-1}\right) P\left(X_{1}, \ldots, X_{t-1}\right) $

    여기서 베이즈 정리에 따르면 왜 Evidence가 없는가?  
    그 이유는 좌, 우항에 $ X_{t} $ 를 분모에 넣어보면, 사후확률로 알고 있는 부분을 보면,  
    $ P\left(X_{1}, \ldots, X_{t}\right) $ 이 부분을 모든 시퀀스의 교집합으로 이해하면 된다.  

2. 과제 코드 리뷰 및 질문
    - 선택 과제 : 2, 3  
      선택 과제 2번의 경우,  
      RNN의 Back Propagation에 대해 코드로 구현하는 것인데,  
      신경망의 점화식은 다음과 같이 구현할 수 있다.

      $ S_k = f(S_{k-1} \cdot W_{rec} + X_k \cdot W_x) $

      그리고 Backpropagation 구현을 위해 Output y에 대한 loss $\xi$ 의 gradient $\partial \xi / \partial y$ 를 계산하는 것으로 시작합니다.
      연쇄 법칙을 활용해서 Gradient값을 구합니다.  

      $ \frac{\partial \xi}{\partial W_x} = \frac{\partial S_k}{\partial W_x} * \frac{\partial \xi}{\partial S_k} = \frac{\partial }{\partial S_k}\xi * X_K$

      $ \frac{\partial \xi}{\partial W_{rec}} = \frac{\partial S_k}{\partial W_{rec}} * \frac{\partial \xi}{\partial S_k} = \frac{\partial }{\partial S_k}\xi * S_{k-1} $

      $ \frac{\partial }{\partial W_x}S_k = \frac{\partial }{\partial S_k}f(S_{k-1} \cdot W_{rec} + X_k \cdot W_x) $

      $ \frac{\partial }{\partial W_{rec}}S_k = \frac{\partial }{\partial W_{rec}}f(S_{k-1} \cdot W_{rec} + X_k \cdot W_x) $

      이를 이용해서 코드를 구현한다.  wx_grad,  wRec_grad 의 코드를 Gradient Descent 를 이용해서 구해준다. back prop이기 때문에 forward를 통해 만들어진 마지막부터  
      첫번째까지 반복문을 돌려주도록 한다.  

3. 알고리즘 문제 :

  - 1주차: 문자열 매칭 알고리즘 (E.g. KMP, 정규표현식)
  - 2주차: 우선순위 큐
  - 3주차: BFS/DFS
  - 4주차: 그래프

### 4. 학습 회고

- 오늘은 선택 과제에 묶여 있었다. 답을 별 것 아닌 것인걸 알았지만, 또 막상 하다보면 계속 막히게 되는 것 같다.  
  이론에 대해서 아는 것도 중요하지만 계속 코드로 작성하면서 익숙해져야겠다.

- 일주일이 지났다. 파이세션의 용도를 더 잘 써야겠다는 생각을 했다. 너무 어색해했고, 그것 때문에 모르는 것에 대해  
  자꾸 막혔는데도 질문을 하지 않았던 것 같다.  
  다음 주부터는 알고리즘 테스트 스터디를 낄 것 같다.  
  할 수 있을 지 의문이 들지만, 최대한 시간을 효율적으로 쓰면서 해야겠다는 생각이 들었다.  
  
- 막힐 때는 딱 30분만 붙잡자. 1시간 이상 붙잡으면 아무것도 못할 것 같다.