---
title: "[ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ ìº í”„] AI-Tech-re - week2(4) - Transformer"
categories: AI boostcourse
tags: python
published: true
use_math: true
---

[RNN](https://hyuns1102.github.io/ai/boostcourse/AItech-9/)ì— ì´ì–´ì„œ Transformerì— ëŒ€í•œ ë‚´ìš©ì´ë‹¤.

#### Transformer

[ì›ë³¸ ë§í¬](https://nlpinkorean.github.io/illustrated-transformer/)ë¥¼ ì°¸ê³ í•´ì„œ ë³´ì.  

ë¬¸ì¥ì˜ Sequenceê°€ ìˆì„ ë•Œ, ë‹¨ì–´ê°€ ëŠê¸°ê±°ë‚˜ ìƒëµë˜ê±°ë‚˜ í•œë‹¤ë©´ ? ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ? ë¼ëŠ” ì˜ë¬¸ì—ì„œ ì‹œì‘ë˜ì—ˆë‹¤.  
ì¸ì½”ë”© ì´ì „ì— linear ëª¨ë¸ì„ í†µê³¼í•œ í›„ì— ë‚˜ì˜¨ vectorë¥¼ ì´ìš©í•´ì„œ ì¸ì½”ë”©ì„ ì‹œì‘í•œë‹¤.  

  ![s1](/assets/images/AI-Images/img71.png)

```python
ğŸ“¢ëª¨ë¸ì˜ í•µì‹¬ì„ ì •ë¦¬í•˜ìë©´,  
Mutil-head Self-Attentionì„ ì´ìš©í•´ì„œ Sequentail Computationì„ ì¤„ì´ê³ ,  
ë§ì€ ë¶€ë¶„ì„ ë³‘ë ¬ì² ë¦¬ê°€ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì–´ì„œ ë” ë§ì€ ë‹¨ì–´ë“¤ ê°„ Dependecyë¥¼ ëª¨ë¸ë§ í•©ë‹ˆë‹¤.  
```

- Overview of Architecture

  black boxë¥¼ ì—´ê¸° ì „ì—, ì „ì²´ì ì¸ ArchitectureëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  

  ![s1](/assets/images/AI-Images/img72.png)

  ìœ„ì˜ TransFormerë¼ëŠ” boxë¥¼ ì—´ì–´ë³´ê²Œ ë˜ë©´, encoding, decodingë¶€ë¶„ë“¤ê³¼ ê·¸ ì‚¬ì´ connectionì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ì—ì„œ ë³´ë“¯ì´ í•˜ë‚˜ì˜ ë¬¸ì¥ì´ Encoderë¥¼ í†µí•´ ë“¤ì–´ê°€ì„œ decoderë¥¼ í†µí•´ ë²ˆì—­ë˜ì–´ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

  ![s1](/assets/images/AI-Images/img73.png)

- Sequence to Sequence Transformer  
  Sequence í•˜ë‚˜ë‹¹ nê°œì˜ ë‹¨ì–´ ëª¨ë‘ë¥¼ ì¸ì½”ë”©í•  ìˆ˜ ìˆë‹¤.  

  a. Nê°œì˜ ë‹¨ì–´ê°€ ì–´ë–»ê²Œ ì¸ì½”ë”ì—ì„œ í•œë°©ì— ì²˜ë¦¬?  
  b. ì¸ì½”ë”ì™€ ë””ì½”ë” ì‚¬ì´ì— ì–´ë–¤ ì •ë³´ ì£¼ê³  ë°›ìŒ?  
  c. ë””ì½”ë”ê°€ ì–´ë–»ê²Œ generalize í•  ìˆ˜ ìˆëŠ”ì§€?  

  - a. Nê°œì˜ ë‹¨ì–´ë¥¼ ì–´ë–»ê²Œ ì¸ì½”ë”ì—ì„œ ì²˜ë¦¬í• ê¹Œ?

    Encoder ë‚´ë¶€ë¥¼ ì¢€ ë” ìì„¸í•˜ê²Œ ë³´ë©´ **Self-Attention**ê³¼ **Feed Forward Neural Network**ë¼ëŠ” 2ê°œì˜ Sub layerë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.  

    ![s1](/assets/images/AI-Images/img49.png)

    Word Embedding  
    ê° ë‹¨ì–´ë“¤ì„ Inputì— ì´ìš©í•˜ê¸° ìœ„í•´ì„œ Embedding Vectorë¡œ ë§Œë“­ë‹ˆë‹¤.  
    [Embedding Algorithms ê´€ë ¨ ë§í¬](https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca)

      ![s1](/assets/images/AI-Images/img74.png)

    Self-attention : 3ê°œì˜ ë‹¨ì–´ì— ëŒ€í•œ 3ê°œì˜ ë²¡í„° -> 3ê°œì˜ ë²¡í„°ë¥¼ ì¶œë ¥í•´ì¤€ë‹¤.  
    ì—¬ê¸°ì„œ self-attentionì€ ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤ì— ëŒ€í•´ dependenciesê°€ ì¡´ì¬í•œë‹¤.  
    ex) self-attention at high-level  

    Feed forward Neural Network : self-attentionì—ì„œ ì¶œë ¥ëœ vectorë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.  

      ![s1](/assets/images/AI-Images/img75.png)

    - Encoding ê³¼ì •  

      "Thinking Machines"ë¼ëŠ” Sentenceê°€ Encoding ë˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

      ![s1](/assets/images/AI-Images/img76.png)
      
      <details>
      <summary>Self-Attention</summary>
      <div markdown="1">

        Self-Attentionì˜ ì²« ìŠ¤í…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

        ![s1](/assets/images/AI-Images/img77.png =50%x50%)

        Embedding vector / Queries / Keys / Values ì˜ ë²¡í„°ë“¤ì„ í•˜ë‚˜ì”© ìƒì„±í•´ì£¼ê³   
        Queries, Keys vectorì‚¬ì´ì˜ ë‚´ì ì„ í†µí•´ Score vectorë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤. (ì–¼ë§ˆë‚˜ ê´€ê³„ ìˆëŠ”ì§€)  

        ![s2](/assets/images/AI-Images/img50.png)

        ê·¸ ë‹¤ìŒ Scoreë¥¼ Normalization í•œ í›„, Softmax í•¨ìˆ˜ë¥¼ í†µí•´ ê³„ì‚°í•œ ë‹¤ìŒì— ê°€ì§€ê³  ìˆëŠ” Value vectorì— ê³±í•´ì¤€ë‹¤. (weighted Sum of the value vectors)  
        ğŸ’¡Normalizationì˜ ì´ìœ ëŠ” ê° Wì˜ dimensionì´ ë†’ì•„ì§ì— ë”°ë¼ ê° ìš”ì†Œë³„ë¡œ í¬ê¸°ê°€ ì»¤ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸

        Query : ë””ì½”ë”ì˜ ì´ì „ ë ˆì´ì–´ hidden state  
        K : ì¸ì½”ë”ì˜ output state  
        V : ì¸ì½”ë”ì˜ output state  

        ![s3](/assets/images/AI-Images/img51.png)

        ì´ ê³¼ì •ì„ í†µí•´ z1 vectorë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤.  
        Key, Query, Value vector ê°ê° ì°¾ì•„ë‚´ëŠ” MLPê°€ ì¡´ì¬í•œë‹¤. ìœ„ì˜ ê³¼ì •ì„ ê°„ë‹¨í•˜ê²Œ ë‚˜íƒ€ë‚´ë©´  

        ![s4](/assets/images/AI-Images/img52.png)

        ì´ êµ¬ì¡°ëŠ” ì…ë ¥ê³¼ ì¶œë ¥ì´ ê³ ì •ë˜ì–´ ìˆëŠ” êµ¬ì¡°ê°€ ì•„ë‹ˆë¼, ì…ë ¥ì— ì–´ë–¤ ë‹¤ë¥¸ Sequenceê°€ ë“¤ì–´ì˜¤ë©´ ì¶œë ¥ì´ ê³„ì† ë‹¬ë¼ì§€ê²Œ ëœë‹¤.  
        ì¦‰, í‘œí˜„ë ¥ì´ ë§ì•„ì§„ë‹¤. -> ë§ì€ computation(ê³„ì‚°) ì´ í•„ìš”  

      </div>
      </details>

    - Multi Headed Attention (MHA)  
      í•˜ë‚˜ì˜ Inputì— Nê°œì˜ attentionì„ ë°˜ë³µí•˜ê²Œ ë˜ë©´ MHAë¼ê³  í•œë‹¤.

      ![s5](/assets/images/AI-Images/img53.png)

      ì´ëŸ¬í•œ MHAë¥¼ í†µí•´ ë‚˜ì˜¨ Z ë²¡í„°ë“¤ì€ concatenatingì„ í•˜ê³  W0ë¼ëŠ” í° ê°€ì¤‘ì¹˜ ë²¡í„°ì— ì˜í•´ output Z ìƒì„±

    - Positional Encoding  
      SequenceëŠ” ì–´ë–»ê²Œ ë“¤ì–´ì˜¤ë“  ê°’ì´ encodingì— ì˜í•´ ë‹¬ë¼ì§ˆ ìˆ˜ê°€ ì—†ë‹¤. (orderì— independent)
      ì–´ë–¤ ë‹¨ì–´ê°€ ì–´ëŠ ìœ„ì¹˜ì— ìˆëŠ” ì§€ ì¤‘ìš”í•˜ê¸°ì— ë„£ì–´ì¤€ë‹¤. (ë²¡í„°ê°’ì— offsetì„ ì¤Œ)

      ![s6](/assets/images/AI-Images/img54.png)

  - b. ì¸ì½”ë”ì™€ ë””ì½”ë” ì‚¬ì´ì— ì–´ë–¤ ì •ë³´ ì£¼ê³  ë°›ìŒ?

      ![s7](/assets/images/AI-Images/img55.png)

      Encoderì—ì„œ Decoderë¡œ ê°ˆ ë•Œ, ì–´ë–¤ ì •ë³´ê°€ ì „ë‹¬ë˜ì–´ì§€ëŠ”ê°€?

      ![s8](/assets/images/AI-Images/img56.png)
      ![s9](/assets/images/AI-Images/img57.png)

      Key Code ì™€ Value vector ê°’ì„ Decoderì— ì „ë‹¬í•´ì„œ previous outputê³¼ í•¨ê»˜ outputì´ ë‚˜ì˜¤ê²Œ ëœë‹¤.

    - Self-attention layer & Encoder-Decoder Attention  
      Masking :  ì´ì „ ë‹¨ì–´ë“¤ì—ë§Œ dependent í•˜ë„ë¡ í•˜ê³ ,.. ë¬´ìŠ¨ ëœ»ì¸ì§€ ì´í•´ ë¶ˆê°€..  

      ![s10](/assets/images/AI-Images/img58.png)
      ![s11](/assets/images/AI-Images/img59.png)

ì‹¤ìŠµ - Vit