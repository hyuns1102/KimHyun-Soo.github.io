---
title: "[네이버 부스트 캠프] AI-Tech-re - week1(4)"
categories: AI boostcourse
tags: python
published: true
use_math: true
---

## 학습기록 - 4

오늘 한 일

- 딥러닝, 확률론, 통계학론, 베이즈 통계학 ppt
- 선택 과제 1 코드 구현
- 피어) 강의 내용 발표, 선택 과제 1 리뷰, Quiz 4,5

### 1. 강의 복습 내용

- 선형 모델, 비선형 모델의 정의

- 확률론 속 조건부 확률과 기대값, 몬테카를로 샘플링

- 모수를 통한 통계 모델의 확률 분포 정의 (표본 평균, 표본 분산)
  최대 가능도 추정법 (MLE), 쿨백 라이블러 발산

- 베이즈 확률
  - 조건부 확률을 기반으로 하여 확률을 갱신시켜 검사율을 만드는 확률 모델  
  -> False Alarm 과 Prescision의 관계
  - 조건부 확률을 쓸 때 주의할 인과 관계

### 2. 고민 내용, 결과 (과제 수행 과정, 결과물 정리)

1. 과정
    Gradient Descent 문제  

    1. `다음 기울기 = 현재 기울기 - lr * 현재 기울기` 이를 기본으로 적절한 기울기를 찾을 때까지  x값을 움직여 기울기 업데이트, 해당 기울기가 적절하게 작아진다면,  
    `np.abs(diff) <= epsilon` 끝내기

    2. 기울기 값을 라이브러리에 있는 diff 함수를 이용해 구하는 것이 아닌, 직접 유도를 통해 구한다.

    3. Linear Regression

        - Linear Regression에서 Gradient_W 와 Gradient_b에 대한 정의는 해당 변수에 대한 편미분 값을 의미한다. 이를 이용해서 iteration 마다 w값을 업데이트 함.  
        error 값은 L-norm2 를 이용해서 구한다.

          ```python
          gradient_w = - 2 * np.mean((train_y - (_y)) * (train_x))
          gradient_b = - 2 * np.mean((train_y - (_y)))
          error = np.linalg.norm(train_y - (np.dot(w, train_x) + b), 2)
          ```

        - 같은 Linear Regression에서 가중치를 늘렸고, gradient 계산 시 유사 무어 역행렬 이용한, X transpose 값을 이용하여 gradient를 구한다.

          ```python
          error = train_y - np.dot(expand_x, beta_gd)
          grad = - np.dot(np.transpose(expand_x), error)
          ```

    4. numpy 연산자

        - *\ asterisk operator : 형태가 동일한 두 행렬의 원소 곱을 의미
        - dot : 내적곱, 첫번째 행렬의 열크기와 두번째 행렬의 행크기가 같아야함
        - matmul(@) : 행렬곱, 내적곱과 행렬에서의 결과는 같지만 matmul은 스칼라 값을 하지는 못한다

    5. SGD
      - mini-batch를 짤 때 y값의 원소를 넣어주지 않고, 식을 넣어서 틀림. 오류를 제대로 찾지 못해 헤맸지만 찾게 되어 해결
      - error 값 : 모두 L2 norm으로 error을 나타낼 수 있을 줄 알았는 데 sgd에서는 오류가 걸린다. 왜 그런지 다음 피어세션 또는, 내일 선택 과제 답안으로 확인

### 3. 피어세션 정리

1. 강의 요약 발표 진행
    - 내용: 딥러닝 이해하기, 확률론, 통계학론, 베이즈 이론

2. 과제 코드 리뷰 및 질문
    - 선택 과제 : 1
    - 필수 퀴즈: 4, 5

3. 코딩 스터디 관련 그룹 생성

4. 강의 요약에서 시간이 많이 소요되었기에 강의 요약 발표 부분에서 중요한 것 위주로 내용 정리
  그리고 20분 내로 학습 요약

### 4. 학습 회고

  임성빈 교수님의 말씀을 통해 다시 의지를 태우게 되었던 것 같다.  

  요약하자면,  

  1. 엔지니어가 되기 위해서 필요 기초 지식인 __선형 대수, 확률론, 통계학__ 을 제대로 알고,  
  2. 지식을 알아도 **코드 베이스 활용 공부** 필수
  3. 수학적 지식이 어렵다면, 용어의 정의를 일단 외우고 익숙해질 때까지 빈도 수를 늘리기  
  4. 그리고 가장 중요한, 엔지니어로써 **빠르게 문제를 확립하고 솔루션으로 연결** 하는 버릇을 기르기
  5. 인공지능 대학원은 새로운 분야에 대해 원하면!
  6. 전문성을 가진 사람 - 논문 구현(codeless), 전문성 평가, 교수님 추천(?)
  7. 질문하자

  책, Dive into Deeplearning / 밑바닥부터 시작하는 딥러닝 / Book of why (역설에 대한 설명)

  어제, 오늘은 후회되는 하루를 보낸 것 같다.  
  짧게 요약해도 될 것을 길게 늘리고, 보기 귀찮다고 대충 흘겨 봐서 제대로 이해하지 못해 4번 넘게 보게되어 시간이 낭비했다.  

  시간을 효율적으로..  
