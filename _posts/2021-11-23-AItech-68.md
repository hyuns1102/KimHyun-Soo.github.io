---
title: "[ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ ìº í”„] AI-Tech - Lv3 ëª¨ë¸ ê²½ëŸ‰í™”(2)"
categories: AI boostcourse
tags: python
published: trues
use_math: true
---

# í•™ìŠµ ê¸°ë¡ - ì¢‹ì€ ëª¨ë¸ê³¼ íŒŒë¼ë¯¸í„° ì°¾ê¸° : AutoML ì´ë¡ 

Conventional DL pipelineì´ ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°? ëª©ì ?  
Configurationì˜ íŠ¹ì„±?, ê²½ëŸ‰í™” ê´€ì ì—ì„œ AutoMLì´ ì–´ë–¤ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€?  
AutoML Pipeline êµ¬í˜„ & í•œê³„ì , í˜„ì‹¤ì ì¸ ì ‘ê·¼?  

## Overview

### Conventional DL Training Pipeline
<br>
"Data Engineering"ì´ë¼ í•˜ë©´,  
ìš°ë¦¬ê°€ ì´ì „ì— í–ˆë“¯ì´ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  ì „ì²˜ë¦¬ë¥¼ ê±°ì¹œ í›„,  
Feature Engineeringì„ ì§„í–‰í•œ í›„ì—, (ì£¼ì–´ì§„ ë°ì´í„°ë¡œ ì˜ˆì¸¡ ëª¨ë¸ì˜ ë¬¸ì œë¥¼ ì˜ í‘œí˜„í•˜ëŠ” featuresë¡œ ë³€í˜•ì‹œí‚¤ëŠ” ê³¼ì •)  
ì í•©í•œ ëª¨ë¸ì„ ì„ ì •í•´ì„œ (ML Algorithm) ì í•©í•œ Hyperparametersë¥¼ ì„ ì •í•©ë‹ˆë‹¤.  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img16.png)

ìœ„ì™€ ê°™ì€ Data Engineeringì„ ê±°ì¹œ í›„, ì í•©í•œ ëª¨ë¸ì˜ Architectureì™€ Hyperparameter (ì¢‹ì€ Configuration) ë¥¼ ìœ„í•´ì„œ Train & Evalutationì„ ë°˜ë³µí•˜ê²Œ ë©ë‹ˆë‹¤.  
ì—¬ê¸°ì„œ ì´ ê³¼ì •ì€ ì‚¬ëŒì´ ëŠì„ì—†ì´ ë°˜ë³µí•˜ê²Œ ë©ë‹ˆë‹¤.  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img17.png)

<br>

### Objectives of AutoML
<br>
ìœ„ì™€ ê°™ì€ ê³¼ì •ë“¤ì€ ê³ ê°ì˜ ìš”êµ¬ì‚¬í•­ì— ì˜í•´ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ê±°ë‚˜ ë˜ ë‹¤ë¥¸ ì˜ˆì¸¡ outputì„ ë‚´ê³  ì‹¶ë‹¤ë©´, ìƒˆë¡­ê²Œ tuningì„ í•˜ê²Œ ë©ë‹ˆë‹¤.  

ì—¬ê¸°ì„œ ì‚¬ëŒë“¤ì´ ì‘ì—…í•´ì•¼í•˜ëŠ” ê³¼ì •ì„ ë¹¼ë‚´ê³ ì í•˜ëŠ” ê²ƒì´ **Automated Machine Learning(AutoML)** ì˜ ëª©í‘œì…ë‹ˆë‹¤.  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img18.png)

AutoMLì˜ ë¬¸ì œ ì •ì˜  
HyperParmeter Sampleì— ëŒ€í•´ì„œ Algorithm, Data Train, Data Validê°€ ì •ì˜ë˜ì–´ ìˆì„ ë•Œ, Lossê°€ ê°€ì¥ ë‚®ì€ Hyperparameterë¥¼ ì •ì˜í•´ì£¼ëŠ” ê²ƒ  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img19.png)

<br>

### Properties of configurations in DL
<br>

DL model Configuration (Architecture, Hyperparameter)ì˜ íŠ¹ì§•

1. ì£¼ìš” íƒ€ì… êµ¬ë¶„

    - Categorical
      - ex) optim : (Adam, AdamW, SGD) module : (Conv, BottleNeck, Residual block, ..)
    - Continuous
      - ex) learning rate, regularizer param, ...
    - Integer
      - ex) batch_size, epochs, .. 

2. â­Conditionalâ­í•œ Configurationì— ë”°ë¼ Search spaceê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

    - Optimizerì˜ sample(e.g.SGD,Adamë“±ë“±)ì— ë”°ë¼ì„œ optimizer parameterì˜ ì¢…ë¥˜, search spaceë„ ë‹¬ë¼ì§  
    (e.g, optimizerì— ë”°ë¥¸ learning rate range ì°¨ì´, SGD:momentum, Adam:alpha,beta1,beta2,..ë“±ë“±)
    - Moduleì˜ sample (e.g.VanillaConv, BottleNeck, InvertedResidual ë“±ë“±)ì— ë”°ë¼ì„œ í•´ë‹¹ moduleì˜ parameterì˜ ì¢…ë¥˜,search spaceë„ ë‹¬ë¼ì§

<br>

### ëª¨ë¸ ê²½ëŸ‰í™” ê´€ì ì—ì„œì˜ AutoML
<br>

ëª¨ë¸ì„ ê²½ëŸ‰í™”í•˜ëŠ” ê²ƒì€ ë‘ê°€ì§€ ê´€ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤.  

ê¸°ì¡´ ëª¨ë¸ ê²½ëŸ‰í™” vs ìƒˆë¡œìš´ ê²½ëŸ‰ ëª¨ë¸ Search

- ê¸°ì¡´ ëª¨ë¸ ê²½ëŸ‰í™” : Pruning, Tensor decomposition, ... -> ì´ëŸ¬í•œ ê¸°ë²•ë“¤ì€ í›„ì²˜ë¦¬ê°€ ìˆì–´ì•¼ ì„±ëŠ¥ì´ ë‚˜ì˜µë‹ˆë‹¤.  
- **ìƒˆë¡œìš´ ê²½ëŸ‰ ëª¨ë¸ Search** : NAS(Neural Architecture Search), Auto

<br><br>

## Basic Concept

### AutoML Pipeline
<br>

ì¼ë°˜ì ì¸ AutoML Pipeline  

Configuration : Backbone, hyperparam ëª¨ë‘ í¬ê´„  
Evaluate Objective : ì–´ë–¤ Taskì— ë”°ë¼ ë‹¤ë¥´ë‹¤? -> ì„±ëŠ¥, ì†ë„ or ì„±ëŠ¥ ë“±ë“± Taskë§ˆë‹¤ ì›í•˜ëŠ” ìˆ˜ì¹˜ ê°’ì„ ì„¤ì •í•´ì¤€ë‹¤.  
Blackbox objective : í‰ê°€í•œ Objectiveê°’ì„ ê°€ì§€ê³  í‰ê°€ ì§€í‘œë¥¼ ìµœëŒ€ë¡œ ë§Œë“œëŠ” ëŒë‹¤ë¥¼ ì°¾ëŠ” ê²ƒ

  ![tmp](/assets/images/AI-Images2/lv3_week3/img20.png)

AutoML Pipeline ì˜ˆì‹œ : Bayesian Optimization (BO)  
Blackbox objective : Surrogate Function -> Acquisition Function  
Surrogate Function : $f(\lambda)$ ë¥¼ ì˜ˆì¸¡í•˜ëŠ” Regression ëª¨ë¸  
Acquisition Function : ë‹¤ìŒ $\lambda$ ë¥¼ ì‹œë„í•˜ëŠ” í•¨ìˆ˜  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img21.png)

<br>

### Bayesian Optimization (with Gaussian Process Regression)
<br>

Surrogate Functionê³¼ Acquisition Functionì„ ìì„¸í•˜ê²Œ ì‚´í´ë³´ë©´,  
3ë²ˆì˜ objective ê³„ì‚° í›„, pointê°€ ì—…ë°ì´íŠ¸ ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
pointê°€ ì—…ë°ì´íŠ¸ ë¨ì— ë”°ë¼ Surrogate modelì´ ì—…ë°ì´íŠ¸ ë©ë‹ˆë‹¤. ê·¸ì— ë”°ë¼, ì ì„ ê³¼ ë³´ë¼ìƒ‰ ì˜ì—­ì´ ì¤„ì–´ë“¤ë©´ì„œ ì¢€ ë” ì •ë°€í•´ì§‘ë‹ˆë‹¤.  
Acquisition functionì€ ë‹¤ìŒ $\lambda$ ë¥¼ ì¶”ì²œí•´ì£¼ëŠ” í•¨ìˆ˜ê°€ ì—…ë°ì´íŠ¸ ë©ë‹ˆë‹¤.  
ìœ„ì™€ ê°™ì€ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img22.png)

ê°„ë‹¨ ì„¤ëª… : Gaussian Process Regression

```
Surrrogate modelì€ ì ì ˆí•œ ê´€ê³„ì„±ì„ ê°€ì§€ëŠ” f(Î») ì„ ì°¾ì•„ì£¼ëŠ” ê²ƒ

Acquisition Functionì€ ì ì ˆí•œ ê´€ê³„ì„± ì†ì—ì„œ ê°€ì¥ ì ì ˆí•œ ìœ„ì¹˜ë¥¼ ì°¾ì•„ì£¼ëŠ” ê²ƒ
```
<br>

### Bayesian Optimization (with Gaussian Process Regression) - Surrogate Model
<br>

- ì¼ë°˜ì ì¸ Regression task : "Estimate the function f fits the data the most closely"  
  Set of train data: (X, Y), Set of the test data : (X*, Y*), $Y \approx f(X)+e$  

- ìš°ë¦¬ê°€ ì•Œê³ ì í•˜ëŠ” íŠ¹ì • ìœ„ì¹˜ì˜ Y* ê°’ì€ ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” X, Y, X* ë“¤ê³¼ (positiveê±´, negativeê±´) ì—°ê´€ì´ ìˆì§€ ì•Šì„ê¹Œ?  
  -> X, Y, X* ê°’ìœ¼ë¡œë¶€í„° Y* ë¥¼ ì¶”ì •, ì—°ê´€ì— ëŒ€í•œ í‘œí˜„ì€ Kernel í•¨ìˆ˜ Kë¡œ í‘œí˜„

- f(x)ë¥¼ "Possible output of the function f at input x"ì¸ "Random variable"ë¡œ ë³´ê³ , ê° r.v.ë“¤ì´ Multivariate Gaussian distribution ê´€ê³„ì— ìˆë‹¤ê³  ê°€ì •  
= í•¨ìˆ˜ë“¤ì˜ ë¶„í¬ë¥¼ ì •ì˜í•˜ê³ , ì´ ë¶„í¬ê°€ Multivariate Gaussian distributionì„ ë”°ë¥¸ë‹¤ ê°€ì •  
= í•¨ìˆ˜ fê°€ **Gaussian process**ë¥¼ ë”°ë¥¸ë‹¤.  

ì‹ì˜ í‘œí˜„ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

![tmp](/assets/images/AI-Images2/lv3_week3/img27.png) 

[ì°¸ê³ ìë£Œ](https://www.edwith.org/bayesiandeeplearning/lecture/24811?isDesc=false)

- ê²°êµ­, ìœ„ ì‹ì˜ í‘œí˜„ì€ ì•Œê³ ì í•˜ëŠ” ê°’(test)ê³¼ ì•Œê³ ìˆëŠ” ê°’(train)ì˜ ê´€ê³„ë¥¼ Multivariate Gaussianì´ë¼ê³  ì •ì˜í•œ ê²ƒì´ë‹¤.  

- Gaussian Identities (Gaussianì˜ marginal, conditionalë„ Gaussian)  
ìœ„ì™€ ê°™ì€ ì •ì˜ë¥¼ ë”°ë¥¼ ë•Œ, Multivariate Gaussianì˜ marginal, conditionalí•œ ê°’ë„ Gaussian ë¶„í¬ë¥¼ ë”°ë¥¼ ìˆ˜ ìˆë‹¤.  
ì´ ì‹ì˜ ì˜ë¯¸ëŠ” ê´€ê³„ê°€ ì •ì˜ê°€ ëœë‹¤ë©´ fì™€ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, f*ì˜ Conditional ë¶„í¬ë¥¼ ì •ì˜í•´ì¤„ ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img23.png)

ê·¸ë¦¼ìœ¼ë¡œ ë³´ì¶© ì„¤ëª…ì„ í•˜ìë©´, Multivariate Gaussianì˜ ê´€ê³„ë¥¼ ê°€ì§ˆ ë•Œ, ì–´ëŠ ìœ„ì¹˜ì—ì„œ ë³´ë”ë¼ë„ ëª¨ë‘ Gaussian Distributionì„ ì´ë£¬ë‹¤.

  ![tmp](/assets/images/AI-Images2/lv3_week3/img24.png)

ê²°ë¡ ì ìœ¼ë¡œ, ì´ì „ì— ì„¤ëª…í–ˆë˜ fì™€ f* ì‚¬ì´ì— ê´€ê³„ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ Multivariate Gaussianìœ¼ë¡œ ì •ì˜ ëœë‹¤ë©´, X, f, X* (X* : ë‹¤ìŒ ë°ì´í„°) ë§Œìœ¼ë¡œ f*|X*ì˜ ë¶„í¬ë¥¼ í‰ê· , ë¶„ì‚°ìœ¼ë¡œ ì •ì˜ë¥¼ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img25.png) 

- Surrogate Model(Function): $f(\lambda)$ ì˜ Regression model

  - ì •ì˜ : Objective $f(\lambda)$ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸  
  ì§€ê¸ˆê¹Œì§€ ê´€ì¸¡ëœ $f(\lambda)$ ê°€ ìˆì„ ë•Œ, ìƒˆë¡œìš´ $\lambda^{ * }$ ì— ëŒ€í•œ objective $f(\lambda^{ * })$ ê°’ì€ ì–¼ë§ˆì¼ê¹Œ?

  - Objectiveë¥¼ estimateí•˜ëŠ” surrogate modelì„ í•™ìŠµ, ë‹¤ìŒ ì¢‹ì€ $(\lambda)$ ë¥¼ ì„ íƒí•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©

  - ëŒ€í‘œì ì¸ Surrogate modelë¡œëŠ” Gaussian Process Regression(GPR) Model  
  (Mean: ì˜ˆì¸¡ fê°’, Var: uncertainty)

  [ì°¸ê³ ìë£Œ2-Visualization](https://distill.pub/2019/visual-exploration-gaussian-processes/)

<br>

### Bayesian Optimization (with Gaussian Process Regression) - Acquisition Function
<br>

Acquisition Function : ë‹¤ìŒì€ ì–´ë””ë¥¼ trialí•˜ë©´ ì¢‹ì„ê¹Œ?

- Surrogate modelì˜ outputìœ¼ë¡œë¶€í„°, ë‹¤ìŒ ì‹œë„í•´ë³´ë©´ ì¢‹ì„ $\lambda$ ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
- Exploration vs Exploitation  
  ("ë¶ˆí™•ì‹¤í•œ ì§€ì  vs ì•Œê³ ìˆëŠ” ê°€ì¥ ì¢‹ì€ ê³³"ì˜ trade off)
  ì´ 2ê°œë¥¼ huristicí•˜ê²Œ (balancing) ì°¾ì•„ì£¼ëŠ” ê²ƒ
- Acquisition functionì˜ max ì§€ì ì„ ë‹¤ìŒ iterì—ì„œ trial
- ex) Upper Confidence Bound(UCB)
  
  ![tmp](/assets/images/AI-Images2/lv3_week3/img26.png) 

<br>

### Bayesian Optimization (with Tree-structured Parzen Estiamtor)  
<br>

Tree-structured Parzen Estimator(TPE)
- GP (Gaussian process)ì˜ ì•½ì :
  - High-dim(O(N**3))
  - Conditional, continuous/discrete íŒŒë¼ë¯¸í„°ë“¤ì˜ í˜¼ì¬ì‹œ ì ìš©ì´ ì–´ë ¤ì›€ -> ë‹¤ë¥¸ í…Œí¬ë‹‰ë“¤ì˜ ì ìš©ì´ í•„ìš”

- TPE : GPR( $p(f \mid \lambda)$ )ì™€ ë‹¤ë¥´ê²Œ $p(\lambda \mid f)$ ì™€ $p(\lambda)$ ë¥¼ ê³„ì‚°

- TPEë¥¼ í†µí•œ ë‹¤ìŒ stepì˜ $\lambda$ ê³„ì‚° ë°©ë²•

  - í˜„ì¬ê¹Œì§€ì˜ observationë“¤ì„ íŠ¹ì • quantile(inverse CDF)ë¡œ êµ¬ë¶„ (ex, ì „ì²´ ì¤‘ 75% bad, 25% good)

  - KDE(Kernel density estimation)ìœ¼ë¡œ good observations ë¶„í¬(p(g)), bad observationsì˜ ë¶„í¬(p(b))ë¥¼ ê°ê° ì¶”ì •

  - p(g)/p(b)ì€ EI(Expected Improvement, acquisition function ì¤‘ í•˜ë‚˜)ì— ë¹„ë¡€í•˜ë¯€ë¡œ([6]), ë†’ì€ ê°’ì„ ê°€ì§€ëŠ” ğ€ë¥¼ ë‹¤ìŒ stepìœ¼ë¡œ ì„¤ì •  
  
  -> ì´ ê³¼ì •ë“¤ì„ í†µí•´ì„œ Surrogate model & Acquisition functionì˜ ê³¼ì •ì„ ë™ì‹œ ìˆ˜í–‰ ê°€ëŠ¥

    ![tmp](/assets/images/AI-Images2/lv3_week3/img28.png)


Tree-structured Parzen Estimator(TPE) : EI ì¦ëª… (EIê°€ ì™œ ë¹„ë¡€í•˜ëŠ”ê°€?)
- Likelihoodë¥¼ Quantileë¡œ êµ¬ë¶„ë˜ëŠ” ë‘ í•¨ìˆ˜ë¡œ ì •ì˜
- ì¦ëª…ì— ëŒ€í•œ ìˆ˜ì‹

    ![tmp](/assets/images/AI-Images2/lv3_week3/img29.png)

    ![tmp](/assets/images/AI-Images2/lv3_week3/img30.png)

```
ìœ„ì˜ ìˆ˜ì‹ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë³´ìë©´,  
l(x)(ì¢‹ì•˜ë˜ ê´€ì¸¡ ë¶„í¬), g(x)(ì•ˆì¢‹ì•˜ë˜ ê´€ì¸¡ ë¶„í¬) ì´ë¼ í•  ë•Œ,  
l(x)/g(x)ê°€ ê°€ì¥ ë†’ì€ ì§€ì ì„ íƒìƒ‰í•˜ëŠ” ê²ƒì€ l(x)ê°€ ë†’ì€ ìª½ì„ ì„ í˜¸í•˜ë˜, g(x)ê°€ ë‚®ì€ ê³³ë„, ì¦‰, ì•ˆì¢‹ì€ì§€ ì•Œ ìˆ˜ ì—†ëŠ” ê²ƒë„ ì°¾ì•„ë³´ìëŠ” ì˜ë¯¸
```
<br>

## Further Studies
<br>

AutoMLì˜ í•œê³„ì ê³¼ í˜„ì‹¤ì ì¸ ì ‘ê·¼ ë°©ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë©´?

- í•˜ë‚˜ì˜ iterë„ êµ‰ì¥íˆ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ë‹¨ì ì´ ì¡´ì¬.

    ![tmp](/assets/images/AI-Images2/lv3_week3/img31.png)

ì—°êµ¬ê°€ í™œë°œí•˜ê²Œ ì§„í–‰ë˜ê³  ìˆë‹¤!

- DLì—ì„œì˜ AutoMLì€ Scalability ì´ìŠˆê°€ ë”ìš± ëŒ€ë‘ë¨
- ì£¼ìš” í‚¤ì›Œë“œ
  - Hyperparameter GD (íƒìƒ‰ê³¼ í•™ìŠµì„ ë™ì‹œì—)
  - Meto-learning(Auto "AutoML")
  - Multi-fidelity optimization -> ì‹œê°„ ì¤„ì´ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²•
    - Dataì˜ subsetë§Œì„ í™œìš©
    - ì ì€ epoch
    - RLì„ í™œìš©í•œ ì ì€ trial (ê°•í™”í•™ìŠµ)
    - Image Downsampling

ì ˆì¶©ì•ˆ

ì¶©ë¶„íˆ ì ˆì¶© ê°€ëŠ¥
  - ì–´ëŠ ì •ë„ì˜ priorë¥¼ ê°œì…, ì ì€ search spaceë¥¼ ì¡ê³ ,
  - ì ì§€ë§Œ, ëŒ€í‘œì„±ì„ ë„ëŠ” ì¢‹ì€ subset ë°ì´í„°ë¥¼ ì •í•˜ê³ (+ n-fold Cross Validation ë“±ì˜ í…Œí¬ë‹‰)
  - í•™ìŠµ ê³¼ì •ì˜ profileì„ ë³´ê³  early terminateí•˜ëŠ” ê¸°ë²• ì ìš©  
    (ASHA Scheduler, BOHB(Bayesian Optimization & Hyperband))

ë“±ë“±ì˜ ë°©ë²•ìœ¼ë¡œ "ì¶©ë¶„íˆ ì¢‹ì€" configurationì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.