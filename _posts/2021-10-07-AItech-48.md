---
title: "[네이버 부스트 캠프] AI-Tech - Lv2 week5(3)"
categories: AI boostcourse
tags: python
published: true
use_math: true
---

## 학습기록 - 46

오늘 할 일  

- Advanced Object Detection 2

## 1. 강의 복습 내용

### M2Det

- Contribution
  - 하나의 GPU에서 훈련할 수 있는 빠르고 정확한 Object Detector
  - BOF, BOS 방법들을 실험을 통해서 증명하고 조합을 찾음
    - BOF (Bag of Freebies) : inference 비용을 늘리지 않고 정확도 향상시키는 방법
    - BOS (Bag of Specials) : inference 비용을 조금 높이지만 정확도가 크게 향상하는 방법  
    ex) Cascade-RCNN or Deformable R-CNN
  - GPU 학습에 더 효율적이고 적합하도록 반영 -> Competition과 같은 리소스가 제한적일 때, 효율적으로 할 수 있다.

- Object Detection Model

    ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img1.png)

  - Input
    - Image, Patches, Image Pyramid, ...
  - Backbone
    - GPU platform : VGG, ResNet, ResNext, DenseNet, ...
    - CPU platform : SqueezeNet, MobileNet, ShuffleNet, ...
  - Neck
    - Additional blocks : SPP, ASPP, ...
    - Path-aggregation blocks : FPN, PAN, NAS-FPN, BiFPN, ...
  - Head
    - Dense Prediction(one-stage) : RPN, YOLO, SSD, RetinaNet, CornerNet, FCOS, ...  
      -> 여기서 RPN은 one-stage model이 아닌 two-stage에서 Region Proposals 영역을 만들어 내는 Network
    - Sparse Prediction(two-stage) : Faster-RCNN, R-FCN, Mask R-CNN, ...

- Related work  
  
  - Bag of Freebies  
  Inference 비용을 늘리지 않고 사용하는 방법, Real-time 보장을 한다면 BOF 방법을 사용한다해서 Real-time 속도가 늘어나지는 않는다.  
  세 가지 파트 - Data Augmentation, Semantic Distribution Bias, Bounding Box Regression

    - Data Augmentation
      - 입력 이미지를 변화시켜서 Overfitting을 막는 방법
        - CutMix

    - Semantic Distribution Bias
      - 데이터셋에 특정 라벨 (배경)이 많은 경우 불균형을 해결하기 위한 방법
        - 어려운 배경을 강제로 배치에 더 많이 포함시키는 방법 (Hard Negative Mining)
        - 어려운 예제에서는 Gradient를 크게, 쉬운 곳은 작게 (Focal-loss)
        - 라벨에 0 또는 1로 설정하는 것이 아니라 smooth하게 부여 (Label smoothing)

    - Bounding Box Regression
      - Bounding box 좌표값들을 예측하는 방법 (MSE) 은 거리가 일정하더라도 IoU 가 다를 수 있음
      - IoU 기반 loss 제안 (IoU 는 1 에 가까울수록 잘 예측한 것이므로 loss 처럼 사용 가능)

      - GIOU
        - IoU 기반 Loss

          ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img2.png)

  - Bag of Specials
    - Enhance receptive field  
      Feature map의 receptive field를 키워서 검출 성능을 높이는 방법  
      - SPP (Spatial Pyramid Pooling)  
      - Shifted Window (in Transformer)  
    - Attention Module  
      Feature map에 Global Attention 추가  
      - SE (A Squeeze and Excitation block) : channel 별로 Global Averaging pooling 연산을 통해 1x1xC로 만든 후, sigmoid 등 여러 연산을 통해서 이전 feature map과의 연산을 통해 channel의 중요 부분에 집중 가능

      - CBAM : Channel Attention + Spatial Attention -> 중요한 부분에 더 집중할 수 있도록 한다.

    - Feature Integration
      - Feature map 통합하기 위한 방법 (FPN : Feature Pyramid Network)
      - Scale wise Feature Aggregation Module (SFPM)

    - Activation Function  
      - ReLU  
        Gradient vanishing 문제 해결하기 위한 활성 함수로 등장  
        음수 값이 나오면 훈련이 되지 않는 현상 발생  

      - Swish / Mish  
        약간의 음수 값을 허용, ReLU의 zero bound 보다 gradient 흐름에 좋은 영향  
        모든 구간에서 미분 가능  

    - Post-processing Method  
      불필요한 Bbox 제거 방법  
      - NMS (Non Maximum Suppression)

  - BoF and BoS for YOLOv4 Backbone (정리)

    ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img3.png)

- Selection of Architecture  
  3가지를 중점에 두고 구조 설계  

  ```python
  - 작은 물체 검출을 위한 큰 네트워크 입력 사이즈  
  - 큰 receptive field 필요 (많은 layer 필요)  
  - 다양한 사이즈의 물체 검출을 위한 모델 용량 (많은 파라미터 필요)  
  ```

  - Overall Architecture  
    ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img4.png)

  - Cross Stage Partial Network (CSPNet)

    ```python
    장점
    정확도 유지하면서 경량화
    메모리 cost 감소
    다양한 backbone 에서 사용가능
    연산 bottleneck 제거
    ```

    - 기존 DenseNet

      ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img5.png)

      문제점은 가중치 업데이트 할 때, Gradient 정보가 반복적으로 쓰이게 된다. 반복적으로 사용되는 Gradient를 방지하기 위해서 일부만 사용할 수 있도록 한다.  

      ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img6.png)

    - CSPDenseNet

      ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img7.png)

      gradient information 많아지는 것 방지, 모든 backbone에 적용 가능

  - Additional Improvements

    - Data Augmentation
      - Mosaic : 4장의 이미지를 합쳐서 사용 -> Batch size가 커지는 효과, 작은 Batch size로도 학습이 잘된다.
      - Self-Adversarial Training  
      1-stage : 원본 이미지 변형, 보이지 않는 이미지 추가  
      2-stage : 변형 이미지 이용  
      모델의 오작동을 일으킨다.  

    - Modified Spatial Attention Module
    - Modified PAN (Path Aggregation Network)
    - Cross mini-Batch Normalization : 각 batch에서 계산하는 것이 아닌 각 mini-batch들을 accumulate 하는 방법을 이용.

      ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img8.png)

      ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img9.png)

  - BoF and BoS for YOLOv4 **backbone**  
  YOLOv4에서 채택한 방법들  
    ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img10.png)

    ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img11.png)

    ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img14.png)

  - BoF and BoS for YOLOv4 **detector**

    ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img12.png)

    ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img13.png)

    ![Untitled](/assets/images/AI-Images2/lv2_week8_2/img15.png)

- 결론  
  BoF, BoS를 이용한 다양한 실험 결과를 보여준다. 하지만 다양한 Dataset에서 공통된 결과가 아니기 때문에, 모든 가능성을 열어두고 많은 실험을 해야한다. 

- 참고  

  [YOLOv4 논문](https://arxiv.org/pdf/2004.10934.pdf)

  [YOLOv4 논문 리뷰](https://hoya012.github.io/blog/yolov4/)

## 2. 고민 내용, 결과 (과제 수행 과정, 결과물 정리)

### Sum of Loss in Model Training

여러 모델들의 loss를 모두 더해줘서 backward()를 하는 이유는? 해도 괜찮은건가?
각각의 loss를 구해서 backward()를 해줘야 하지 않는건가?

  ![Untitled](/assets/images/AI-Images2/lv2_week8_3/img1.png)

---

### Sol.

Document에서 보면, " Autograd를 하면서 gradients를 계산하고 각 모델 Parameter들의 .grad attribute 안에 저장한다. "

  ![Untitled](/assets/images/AI-Images2/lv2_week8_3/img2.png)

"tensor a와  tensor b에 대해서 Q를 다음과 같이 가정했을 때, backward()를 수행한다면, 다음과 같이 편미분이 일어나고 autograd는 .grad attribute를 가지고 있는 각각의 tensor에 대해 따로 수행되고 저장된다."

  ![Untitled](/assets/images/AI-Images2/lv2_week8_3/img3.png)

  [A Gentle Introduction to torch.autograd - PyTorch Tutorials 1.9.1+cu102 documentation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#usage-in-pytorch)  

+++ 추가 설명,  

위의 내용을 코드로 구현하면,  

1) loss_1, loss_2에 대해서 backward를 진행했을 때,

    ```python
    import torch
    import torch.nn as nn

    pred_y1 = torch.tensor([1.], requires_grad=True)
    y1 = torch.tensor([5.], requires_grad=True)
    pred_y2 = torch.tensor([2.], requires_grad=True)
    y2 = torch.tensor([4.], requires_grad=True)

    loss_1 = y1**2 - pred_y1**2
    loss_2 = y2**3 - pred_y2**3

    print(loss_1)
    #tensor([24.], grad_fn=<SubBackward0>)
    print(loss_2)
    #tensor([56.], grad_fn=<SubBackward0>)

    # loss_1에 대해서 backward을 했을 때,
    loss_1.backward()

    # 다음과 같이, backward를 실시한다면, 각 tensor안에 grad attribute안에
    # loss_1에서 y1, pred_y1에 대한 편미분 값을 저장하게 된다.

    print(pred_y1.grad)
    #tensor([-2.])
    print(y1.grad)
    #tensor([10.])

    loss_2.backward()

    # loss_2에 대해 backward
    print(pred_y2.grad)
    #tensor([-12.])
    print(y2.grad)
    #tensor([48.])

    # 각각 backward를 진행했을 때, 각 텐서의 grad 안에 미분 값을 저장하게 됩니다.
    ```

2) loss_1, loss_2에 대해서 backward를 진행했을 때,

    ```python
    import torch
    import torch.nn as nn

    pred_y1 = torch.tensor([1.], requires_grad=True)
    y1 = torch.tensor([5.], requires_grad=True)
    pred_y2 = torch.tensor([2.], requires_grad=True)
    y2 = torch.tensor([4.], requires_grad=True)

    loss_1 = y1**2 - pred_y1**2
    loss_2 = y2**3 - pred_y2**3

    print(loss_1)
    #tensor([24.], grad_fn=<SubBackward0>)
    print(loss_2)
    #tensor([56.], grad_fn=<SubBackward0>)
    loss = loss_1 + loss_2
    print(loss)
    # tensor([80.], grad_fn=<AddBackward0>)

    # loss = loss_1 + loss_2에 대해서 backward()를 했을 때,
    loss.backward()

    # 각 loss에서 진행했던 backward() 값과 동일하게 나옵니다.
    print(pred_y1.grad)
    #tensor([-2.])
    print(y1.grad)
    #tensor([10.])
    print(pred_y2.grad)
    #tensor([-12.])
    print(y2.grad)
    #tensor([48.])

    # 즉, loss를 합해도 동일한 gradient를 가지게 됩니다.
    ```

여기까지 정리하면,  

각 텐서를 가진 채로 연산을 수행할 때, requires_grad 에 따라서 grad attribute가 생성됩니다.  
loss에 대해서 backward를 진행할 때, 각 텐서에 존재하는 grad라는 attribute안에 미분 값(변화도)이 저장됩니다.  

그런데 loss는 tensor가 갖고 있는 모든 값을 더한 값인 scalar 형태로 output이 있어야 한다고 합니다.  

왜 그런 것일까요?  

위의 예시에서 tensor값을 vector로 늘렸을 때,  

  ```python
  import torch
  import torch.nn as nn
  
  pred_y1 = torch.tensor([1., 2., 3.], requires_grad=True)
  y1 = torch.tensor([5., 6., 7.], requires_grad=True)
  
  loss_1 = y1**2 - pred_y1**2
  
  print(loss_1)
  #tensor([24., 32., 40.], grad_fn=<SubBackward0>)
  
  loss_1.backward()
  # RuntimeError: grad can be implicitly created only for scalar outputs
  ```

다음과 같이 loss_1은 Runtime Error가 나게 됩니다.  
하지만, 아래와 같이 모든 값을 더해주고 backward()를 하게 된다면 각 vector에 gradient 값이 전달됩니다.  

  ```python
  loss_1 = (y1**2 - pred_y1**2).sum()
  loss_1.backward()
  
  print(pred_y1.grad)
  #tensor([-2., -4., -6.])
  print(y1.grad)
  #tensor([10., 12., 14.])
  ```

왜 그런 것일까요?

처음엔, loss_1을 sum하지 않아도 loss_1 = [ 24., 32., 40.]  에 대해 각각 미분이 적용되지 않을까? 라는 생각을 했습니다.

해답은 **torch.autograd 는 Jacobian 연산을 위한 엔진**이기 때문이었습니다. Jacobian product를 이용해서 각 해당 tensor 안에 값들에 대한 추적이 가능합니다.
 → 조금 더 자세하게 설명하고 싶은데.. Jacobian에 대한 이해가 머릿속으로 잘 그려지지 않네요.. 

Jacobian 관련 참고자료

[How Pytorch Backward() function works - 링크](https://mustafaghali11.medium.com/how-pytorch-backward-function-works-55669b3b7c62)

## 3. 피어세션 정리

## 4. 학습 회고

- 