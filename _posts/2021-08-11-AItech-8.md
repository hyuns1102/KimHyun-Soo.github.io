---
title: "[네이버 부스트 캠프] AI-Tech-re - week2(3)"
categories: AI boostcourse
tags: python
published: true
use_math: true
---

## 학습기록 - 8

오늘 한 일

- Convolution은 무엇인가? / CNN Assignment
- 백준 알고리즘 (IOIOI)
- git 강의 by egoing 님 실습 - [참고 링크](https://www.opentutorials.org/course/2708)
- 피어) CNN 관련 내용 발표, 알고리즘 스터디

### 1. 강의 복습 내용

#### CNN

1. CNN (Convolutional Neural Network)
    Convolution은 두가지 함수의 합성곱의 적분을 뜻하며, 이미지에서의 컨볼루션은 필터를 통해 새로운 이미지를 출력하게 된다.  
    아래와 같이 이미지에 대한 컨볼루션 연산을 나타낼 수 있다.  

    ![s1](/assets/images/AI-Images/img32.png)

2. RGB Convolution
    RGB Convolution의 경우, 이미지 값이 총 3개의 dimension으로 이루어져 있을 때, 똑같은 3개의 dimension의 커널 4개를 컨볼루션 시켜주면  
    4개의 feature가 만들어지게 된다.  (32X32X3  *  5X5X3  -> 28X28X1 )

    ![s2](/assets/images/AI-Images/img33.png)

3. 고전적인 CNN
    CNN은 다음과 같이 3가지 layer들로 이루어져 있다. 각각의 역할을 통해 이미지의 분류가 가능하다.  
    요즘 추세는 Fully Connected layer를 줄임으로써 Parameter의 숫자를 낮춰주도록 한다.  
    성능을 높이기 위해서는 Network의 깊이를 깊게 만드는 것도 중요하지만,  
    Parameter의 숫자를 낮춰 연산량을 낮추는 것이 중요하다.

    ![s3](/assets/images/AI-Images/img34.png)

4. Stride
    얼마나 dense하게 찍을 것인가?

    ![s4](/assets/images/AI-Images/img35.png)

    Padding
    가장 자리에 덧대줘서 모든 input이 convolution을 가능하게 만든다.

    ![s5](/assets/images/AI-Images/img36.png)

5. Convolution Arithmetic

    우리가 Padding(1), Stride(1), 3X3 커널에서 Input이 40X50X128 일 때,  
    Output을 40X50X64로 만들 때의 파라미터의 개수? -> 각각 네트워크의 모양만 봐도 '단위' 개념에서 감이 생겨야 한다.  

    ![s6](/assets/images/AI-Images/img37.png)

6. Alexnet

    Convolution layer, Dense layer? ( MLP or Fully Connected layer )
    Fully connected layer : 완전히 연결 된 layer, 한 층의 모든 뉴런이 다음층의 모든 뉴런과 연결된 상태, 2차원의 배열 형태 이미지를 1차원의 평탄화 작업을 통해 이미지 분류  
    -> 가장 많은 파라미터가 필요함 -> 목표는 줄이는게 가장 필요함.

    ![s7](/assets/images/AI-Images/img38.png)

7. **1X1 Convolution ( 중요 )**

    Fully connected layer에서 input의 width, height를 일치시킨 후 채널이 개수를 늘려 convolution을 진행했다.  
    하지만 이와 같은 방법은 Parameter의 수가 Million까지 가기 때문에 비효율적이다.  
    그래서 1 by 1 Convolution을 통해, dimension을 줄이게 만든다. 그리고 이 방법을 통해 Network의 깊이는 늘리고 Parameter의 수를 줄어들게 해준다.  

    ![s8](/assets/images/AI-Images/img39.png)

### 2. 고민 내용, 결과 (과제 수행 과정, 결과물 정리)

1. 필수 과제 : CNN

    강의 따라 실습 진행, 다시 혼자 보면서 진행해봐야겠다.  
    코드에 대해 완벽히 이해하지 못했다. Convolution을 진행해야한다는 것뿐  

    계속 이런 에러가 떴었는데,  
    ``` CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)` ```
    ``` RuntimeError: cuda runtime error (2) : out of memory at ```
    stack over flower를 통해 해결, 런타임을 다시 시작해서 GPU를 재부팅(?) 해줘야 했었다.  

### 3. 피어세션 정리

### 4. 학습 회고

- 알고리즘에서 한시간, 한시간 CNN 강의, 1시부터 3시반까지 git 강의, 3시반부터 4시반 git 놓친 부분 혼자 다시 이해  
  4시반부터 5시반 피어세션, 5시반부터 6시 50분 멘토링 시간, 7시부터 9시반 저녁 & 잠, 10시~12시 학습 정리 및 회고  
  오늘 일정이 이랬다.  

- 나만의 공부시간에서 이론에 대한 이해가 많이 부족한 것을 느꼈다. 그리고 그것을 바로 해결하는 시간 또한 많이 늦었다.  
  일정을 살펴보자면,

  - 알고리즘에서 내가 했던 방식이 **틀렸다는 것을 인지하고 다른 방법까지 생각하는 데에 1시간**이 걸렸다.  
    하지만 그것 또한 틀린 부분을 해결하지 못해, 시간이 없어서 결국 답을 찾게 되었다.  

  - 강의는 들으면서 필기를 했는데, 이해안되는 부분에서 멈춰서 다시 들었다.  
    한 강의 당 적어도 **두시간 이상이 필요**하고 질문 정리 또한 계속 해야한다고 생각했다.  

  - Git 강의는 처음에 굉장히 천천히 하길래 **집중하지 못했다**. 그래서 한번의 클릭 실수로 마지막 실습이 틀어졌다.  
    이 **놓친 부분을 끝내려고** 실습을 제대로 못들었다. 그리고 끝나고 놓친 부분부터 **원인을 파악하고 해결하는 데까지 한시간 가량** 걸렸다.  

  - 피어세션동안 질문을 했어야 했는데, 제대로 강의에 대한 **질문 정리**가 안되었다. git에 대한 질문 또한 정리를 못해서 쭈뼛거리는 것 때문에 오래 걸렸다.  

  - 멘토링 시간동안 나의 목표, 달성 정도에 대해 생각을 해보았다. 제대로 이루지 못했다는 생각을 했고, 노력해야할 부분은 많지만 계획을 제대로 못잡는다고 생각했다.  
    무엇보다 **내 능력에 비해 계획의 수준이 너무 높다**고 생각했다.  

  - 회고를 할 때 다시 강의를 보게 되고 복습이 아닌 것이 되는 느낌.

- 굵게 표시된 부분을 토대로 나의 문제와 해결법을 정리해보자면,  

    1. 계획 타이트 & 시간 소요가 심하다. -> 8시반 부터 계획 시작 & 타이머 이용
    2. 강의에 대한 집중력 부족. -> 정 안되면 강사님의 모든 말을 받아쓰기라도 하면서 집중하자.
    3. 질문 정리 -> 강의를 들으면서 노트에 계속 왜?를 생각하자.
    4. 틀리면 과감하게 넘어가자.