---
title: "[네이버 부스트 캠프] AI-Tech-re - week2(4) - Transformer"
categories: AI boostcourse
tags: python
published: true
use_math: true
---

[RNN](https://hyuns1102.github.io/ai/boostcourse/AItech-9/)에 이어서 Transformer에 대한 내용이다.

#### Transformer

[원본 링크](https://nlpinkorean.github.io/illustrated-transformer/)를 참고해서 보자.  

문장의 Sequence가 있을 때, 단어가 끊기거나 생략되거나 한다면 ? 어떻게 해야할까? 라는 의문에서 시작되었다.  
인코딩 이전에 linear 모델을 통과한 후에 나온 vector를 이용해서 인코딩을 시작한다.  

  ![s1](/assets/images/AI-Images/img71.png)

```python
📢모델의 핵심을 정리하자면,  
Mutil-head Self-Attention을 이용해서 Sequentail Computation을 줄이고,  
많은 부분을 병렬철리가 가능하게 만들어서 더 많은 단어들 간 Dependecy를 모델링 합니다.  
```

- Overview of Architecture

  black box를 열기 전에, 전체적인 Architecture는 다음과 같습니다.  

  ![s1](/assets/images/AI-Images/img72.png)

  위의 TransFormer라는 box를 열어보게 되면, encoding, decoding부분들과 그 사이 connection이 존재합니다. 이미지에서 보듯이 하나의 문장이 Encoder를 통해 들어가서 decoder를 통해 번역되어 나오는 것을 볼 수 있습니다.  

  ![s1](/assets/images/AI-Images/img73.png)

- Sequence to Sequence Transformer  
  Sequence 하나당 n개의 단어 모두를 인코딩할 수 있다.  

  a. N개의 단어가 어떻게 인코더에서 한방에 처리?  
  b. 인코더와 디코더 사이에 어떤 정보 주고 받음?  
  c. 디코더가 어떻게 generalize 할 수 있는지?  

  - a. N개의 단어를 어떻게 인코더에서 처리할까?

    Encoder 내부를 좀 더 자세하게 보면 **Self-Attention**과 **Feed Forward Neural Network**라는 2개의 Sub layer를 가지고 있습니다.  

    ![s1](/assets/images/AI-Images/img49.png)

    Word Embedding  
    각 단어들을 Input에 이용하기 위해서 Embedding Vector로 만듭니다.  
    [Embedding Algorithms 관련 링크](https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca)

      ![s1](/assets/images/AI-Images/img74.png)

    Self-attention : 3개의 단어에 대한 3개의 벡터 -> 3개의 벡터를 출력해준다.  
    여기서 self-attention은 나머지 단어들에 대해 dependencies가 존재한다.  
    ex) self-attention at high-level  

    Feed forward Neural Network : self-attention에서 출력된 vector를 독립적으로 추출합니다.  

      ![s1](/assets/images/AI-Images/img75.png)

    - Encoding 과정  

      "Thinking Machines"라는 Sentence가 Encoding 되는 과정입니다.

      ![s1](/assets/images/AI-Images/img76.png)
      
      <details>
      <summary>Self-Attention</summary>
      <div markdown="1">

        Self-Attention의 첫 스텝은 다음과 같습니다.

        ![s1](/assets/images/AI-Images/img77.png =50%x50%)

        Embedding vector / Queries / Keys / Values 의 벡터들을 하나씩 생성해주고  
        Queries, Keys vector사이의 내적을 통해 Score vector를 만들어준다. (얼마나 관계 있는지)  

        ![s2](/assets/images/AI-Images/img50.png)

        그 다음 Score를 Normalization 한 후, Softmax 함수를 통해 계산한 다음에 가지고 있는 Value vector에 곱해준다. (weighted Sum of the value vectors)  
        💡Normalization의 이유는 각 W의 dimension이 높아짐에 따라 각 요소별로 크기가 커질 수 있기 때문

        Query : 디코더의 이전 레이어 hidden state  
        K : 인코더의 output state  
        V : 인코더의 output state  

        ![s3](/assets/images/AI-Images/img51.png)

        이 과정을 통해 z1 vector를 만들어낸다.  
        Key, Query, Value vector 각각 찾아내는 MLP가 존재한다. 위의 과정을 간단하게 나타내면  

        ![s4](/assets/images/AI-Images/img52.png)

        이 구조는 입력과 출력이 고정되어 있는 구조가 아니라, 입력에 어떤 다른 Sequence가 들어오면 출력이 계속 달라지게 된다.  
        즉, 표현력이 많아진다. -> 많은 computation(계산) 이 필요  

      </div>
      </details>

    - Multi Headed Attention (MHA)  
      하나의 Input에 N개의 attention을 반복하게 되면 MHA라고 한다.

      ![s5](/assets/images/AI-Images/img53.png)

      이러한 MHA를 통해 나온 Z 벡터들은 concatenating을 하고 W0라는 큰 가중치 벡터에 의해 output Z 생성

    - Positional Encoding  
      Sequence는 어떻게 들어오든 값이 encoding에 의해 달라질 수가 없다. (order에 independent)
      어떤 단어가 어느 위치에 있는 지 중요하기에 넣어준다. (벡터값에 offset을 줌)

      ![s6](/assets/images/AI-Images/img54.png)

  - b. 인코더와 디코더 사이에 어떤 정보 주고 받음?

      ![s7](/assets/images/AI-Images/img55.png)

      Encoder에서 Decoder로 갈 때, 어떤 정보가 전달되어지는가?

      ![s8](/assets/images/AI-Images/img56.png)
      ![s9](/assets/images/AI-Images/img57.png)

      Key Code 와 Value vector 값을 Decoder에 전달해서 previous output과 함께 output이 나오게 된다.

    - Self-attention layer & Encoder-Decoder Attention  
      Masking :  이전 단어들에만 dependent 하도록 하고,.. 무슨 뜻인지 이해 불가..  

      ![s10](/assets/images/AI-Images/img58.png)
      ![s11](/assets/images/AI-Images/img59.png)

실습 - Vit