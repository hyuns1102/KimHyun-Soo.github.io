---
title: "[네이버 부스트 캠프] AI-Tech - Lv3 모델 경량화(1)"
categories: AI boostcourse
tags: python
published: trues
use_math: true
---

## 학습 기록

### 경량화의 목적

경량화가 왜 필요하지에 대해 설명

1) On device AI

- 요즘엔 Smarth Phone, Watch 등 다양한 기기에 딥러닝 모델이 올라가 있다.
- Power, Memory, Storage 에 대한 제약 존재

2) AI on cloud(or server)

- Power, Memory, Storage의 제약이 줄어드나 Latency, throughput의 제약 존재
- 같은 자원으로 Latency, throughput의 필요

3) Computation as a key component of AI progress

- AI 모델 학습에 들어가는 연산이 3,4개월마다 두배로 증가
- 2012년 기준, 2018년에 300,000x 증가
- 연산 측정 방법 : 1) Counting operations(FLOPS) 2) GPU times

경량화는?

- 모델의 연구와는 별개로, 산업에 적용되기 위해서 거쳐야하는 과정
- 요구조건 (하드웨어 종류, latency 제한, 요구 throughput, 성능)들 간의 trade-off를 고려하여 모델 경량화/최적화를 수행

대표 종류

- 네트워크 구조
  - Efficient Architecture Desingn : Module (EfficientNet, ResNet, VGG) + (AutoML;NAS)
  - Network Pruning : 가지치기 같은 것, 중요도가 낮은 Parameter 제거
  - Knowledge Distilation : Teacher, Student Backprop
  - Matrix/Tensor Decomposition : Convolution weight Tensor를 더 작은 단위로 표현 (Approximation)

- Hardware 관점
  - Network Quantization : 더 작은 데이터 타입으로 mapping (int -> float or float -> int)
  - Network Compiling : 기준 Hardware에서 Network를 Compile

### 경량화 분야 소개

1. Efficient architecture design

  ![tmp](/assets/images/AI-Images2/lv3_week3/img1.png)

  Software 1.0: 사람이 짜는 모듈
  Software 2.0: 알고리즘이 찾는 모듈

  사람의 직관보다 조금 더 나은 Architecture를 찾아낸다. (AutoML, NAS)

  ![tmp](/assets/images/AI-Images2/lv3_week3/img2.png)

  ![tmp](/assets/images/AI-Images2/lv3_week3/img3.png)

2. Network Pruning

  - 중요도가 낮은 파라미터를 제ㅓ
  - 좋은 중요도의 정의, 찾는 것이 주요 연구 토픽 중 하나(L2-norm)
  - structured/unstructured pruning으로 나눠짐 

  - Structured Pruning의 경우, 그룹 (filter) 단위로 수행
    - Dense computation에 최적화된 소프트웨어 또는 하드웨어에 적합한 기법 

      ![tmp](/assets/images/AI-Images2/lv3_week3/img4.png)

  - Untructure Pruning의 경우, 파라미터 각각을 독립적으로 pruning하는 기법
    - Pruning을 수행할수록 네트워크 내부의 행렬이 점차 sparse 해짐
    - Sparse Computation에 최적화된 소프트웨어 또는 하드웨어에 적합

      ![tmp](/assets/images/AI-Images2/lv3_week3/img5.png)

3. Knowledge distilation

  - 학습된 큰 네트워크를 작은 네트워크의 학습 보조로 사용하는 방법
  - Soft targets(soft outputs)에는 ground truth 보다 더 많은 정보를 담고 있음
    - 하나의 label로 정해지는 것이 아닌 확률상으로 유사도를 지정해줌

    ![tmp](/assets/images/AI-Images2/lv3_week3/img6.png)

  - 1) Student network와 ground truth label의 cross-entropy  
    2) teacher network와 student network의 inference 결과에 대한 KLD loss로 구성 

    `T는 large teachre network의 출력을 smoothing(soften)하는 역할을 한다. $\alpha$ 는 두 loss의 균형을 조절하는 파라미터다.`

    ![tmp](/assets/images/AI-Images2/lv3_week3/img7.png)

4. Matrix/Tensor decomposition

  - 하나의 Tensor를 작은 tensor들의 operation들의 조합(합, 곱)으로 표현하는 방법
  - Cp decomposition : rank 1 vector들의 outer product의 합으로 tensor를 approximation
  - SVD가 뭘까요..?
  - 아래의 그림은 Full Convolution을 작은 tensor들로 쪼개서 합으로 표현하는 방식

    ![tmp](/assets/images/AI-Images2/lv3_week3/img8.png)

5. Network Quantization

  - 일반적인 float32 데이터 타입의 Network의 연산과정을 그보다 작은 크기의 데이터 타입(float16, int8, ...)으로 변환하여 연산 수행
  - Quantization 는 어느정도 Robust하게 적용한다.

    ![tmp](/assets/images/AI-Images2/lv3_week3/img9.png)

  - 사이즈: 감소
  - 성능 : 일반적으로 약간 하락
  - 속도 : Hardware 지원 여부 및 사용 라이브러리에 따라 다름
  - 정량적이진 않아도 정성적으로 이런 효과를 가짐

    ![tmp](/assets/images/AI-Images2/lv3_week3/img10.png)

6. Network Compiling

  - 학습이 완료된 Network를 deploy하려는 target hardware에서 inference가 가능하도록 compile하는 것 (+최적화 동반)
  - 속도에 가장 큰 영향을 미치는 방법
  - e.g. TensorRT(NVIDIA), Tflite(Tensorflow), TVM(apache), ...
  - Compile library 마다 성능 차이가 발생

    ![tmp](/assets/images/AI-Images2/lv3_week3/img11.png)

  - Compile 과정에서 layer fusion (graph optimization)등의 최적화가 수행 (Rule base)

    ![tmp](/assets/images/AI-Images2/lv3_week3/img12.png)

  - Framework와 hardware backends 사이의 수많은 조합
  - HW마다 지원되는 core, unit 수, instruction set, 가속 라이브러리 등이 다름
  - Goal : Deploy Deep Learning Everywhere
  - 조합에 따라 성능 차이가 많이 발생함.

    ![tmp](/assets/images/AI-Images2/lv3_week3/img13.png)

    ![tmp](/assets/images/AI-Images2/lv3_week3/img14.png)

  - AutoML로 graph의 좋은 fusion을 찾아내자
  - AutoTVM(Apache)

    ![tmp](/assets/images/AI-Images2/lv3_week3/img15.png)

### 강의 목표

- **AutoML**
- **Matrix/Tensor Decomposition**
- **Network Quantization**
- **Network Compiling**

```python

경량 모델 탐색 (AutoML)  
-> 찾은 모델 쪼개기 (Tensor Decomposition)  
-> 쪼갠 모델 꾸겨넣기 (Quantization & Compile)

```

## 프로젝트 - "초" 경량 이미지 분류하기

- 제공받은 모델 돌려보기

## 프로젝트 - 주제 선정