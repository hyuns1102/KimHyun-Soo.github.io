---
title: "[네이버 부스트 캠프] AI-Tech - Lv3 모델 경량화(2)"
categories: AI boostcourse
tags: python
published: trues
use_math: true
---

## 학습 기록

## 좋은 모델과 파라미터 찾기 : AutoML 이론

Conventional DL pipeline이 어떤 문제를 해결? 목적?
Configuration의 특성?, 경량화 관점에서 AutoML이 어떤 의미를 가지는지?
AutoML Pipeline 구현 & 한계점, 현실적인 접근?

### Overview

#### Conventional DL Training Pipeline

"Data Engineering"이라 하면,  
우리가 이전에 했듯이 데이터를 정제하고 전처리를 거친 후,  
Feature Engineering을 진행한 후에, (주어진 데이터로 예측 모델의 문제를 잘 표현하는 features로 변형시키는 과정)  
적합한 모델을 선정해서 (ML Algorithm) 적합한 Hyperparameters를 선정합니다.  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img16.png)

위와 같은 Data Engineering을 거친 후, 적합한 모델의 Architecture와 Hyperparameter (좋은 Configuration) 를 위해서 Train & Evalutation을 반복하게 됩니다.  
여기서 이 과정은 사람이 끊임없이 반복하게 됩니다.  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img17.png)

#### Objectives of AutoML

위와 같은 과정들은 고객의 요구사항에 의해 새로운 데이터가 들어오거나 또 다른 예측 output을 내고 싶다면, 새롭게 tuning을 하게 됩니다.  

여기서 사람들이 작업해야하는 과정을 빼내고자 하는 것이 **Automated Machine Learning(AutoML)** 의 목표입니다.  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img18.png)

AutoML의 문제 정의  
HyperParmeter Sample에 대해서 Algorithm, Data Train, Data Valid가 정의되어 있을 때, Loss가 가장 낮은 Hyperparameter를 정의해주는 것  

  ![tmp](/assets/images/AI-Images2/lv3_week3/img19.png)

#### Properties of configurations in DL

DL model Configuration (Architecture, Hyperparameter)의 특징

1. 주요 타입 구분

  - Categorical
    - ex) optim : (Adam, AdamW, SGD) module : (Conv, BottleNeck, Residual block, ..)
  - Continuous
    - ex) learning rate, regularizer param, ...
  - Integer
    - ex) batch_size, epochs, .. 

2. ⭐Conditional⭐한 Configuration에 따라 Search space가 달라질 수 있습니다.  

  - Optimizer의 sample(e.g.SGD,Adam등등)에 따라서 optimizer parameter의 종류, search space도 달라짐 (e.g, optimizer에 따른 learning rate range 차이, SGD:momentum, Adam:alpha,beta1,beta2,..등등)
  - Module의 sample (e.g.VanillaConv, BottleNeck, InvertedResidual 등등)에 따라서 해당 module의 parameter의 종류,search space도 달라짐

#### 모델 경량화 관점에서의 AutoML

모델을 경량화하는 것은 두가지 관점이 존재합니다.  

기존 모델 경량화 vs 새로운 경량 모델 Search

- 기존 모델 경량화 : Pruning, Tensor decomposition, ... -> 이러한 기법들은 후처리가 있어야 성능이 나옵니다.  
- **새로운 경량 모델 Search** : NAS(Neural Architecture Search), Auto
 
### Basic Concept




