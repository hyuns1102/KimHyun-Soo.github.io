---
title: "[ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ ìº í”„] AI-Tech-re - week2(4) - Transformer"
categories: AI boostcourse
tags: python
published: true
use_math: true
---

[RNN](https://hyuns1102.github.io/ai/boostcourse/AItech-9/)ì— ì´ì–´ì„œ Transformerì— ëŒ€í•œ ë‚´ìš©ì´ë‹¤.

#### Transformer

[ì›ë³¸ ë§í¬](https://nlpinkorean.github.io/illustrated-transformer/)ë¥¼ ì°¸ê³ í•´ì„œ ë³´ì.  

ë¬¸ì¥ì˜ Sequenceê°€ ìˆì„ ë•Œ, ë‹¨ì–´ê°€ ëŠê¸°ê±°ë‚˜ ìƒëµë˜ê±°ë‚˜ í•œë‹¤ë©´ ? ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ? ë¼ëŠ” ì˜ë¬¸ì—ì„œ ì‹œì‘ë˜ì—ˆë‹¤.  
ì¸ì½”ë”© ì´ì „ì— linear ëª¨ë¸ì„ í†µê³¼í•œ í›„ì— ë‚˜ì˜¨ vectorë¥¼ ì´ìš©í•´ì„œ ì¸ì½”ë”©ì„ ì‹œì‘í•œë‹¤.  

  ![s1](/assets/images/AI-Images/img71.png)

```python
ğŸ“¢ëª¨ë¸ì˜ í•µì‹¬ì„ ì •ë¦¬í•˜ìë©´,  
Mutil-head Self-Attentionì„ ì´ìš©í•´ì„œ Sequentail Computationì„ ì¤„ì´ê³ ,  
ë§ì€ ë¶€ë¶„ì„ ë³‘ë ¬ì² ë¦¬ê°€ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì–´ì„œ ë” ë§ì€ ë‹¨ì–´ë“¤ ê°„ Dependecyë¥¼ ëª¨ë¸ë§ í•©ë‹ˆë‹¤.  
```

- Overview of Architecture

  black boxë¥¼ ì—´ê¸° ì „ì—, ì „ì²´ì ì¸ ArchitectureëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  

  ![s1](/assets/images/AI-Images/img72.png)

  ìœ„ì˜ TransFormerë¼ëŠ” boxë¥¼ ì—´ì–´ë³´ê²Œ ë˜ë©´, encoding, decodingë¶€ë¶„ë“¤ê³¼ ê·¸ ì‚¬ì´ connectionì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ì—ì„œ ë³´ë“¯ì´ í•˜ë‚˜ì˜ ë¬¸ì¥ì´ Encoderë¥¼ í†µí•´ ë“¤ì–´ê°€ì„œ decoderë¥¼ í†µí•´ ë²ˆì—­ë˜ì–´ ë‚˜ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

  ![s1](/assets/images/AI-Images/img73.png)

- Sequence to Sequence Transformer  
  Sequence í•˜ë‚˜ë‹¹ nê°œì˜ ë‹¨ì–´ ëª¨ë‘ë¥¼ ì¸ì½”ë”©í•  ìˆ˜ ìˆë‹¤.  

  <details>
  <summary> a. Nê°œì˜ ë‹¨ì–´ê°€ ì–´ë–»ê²Œ ì¸ì½”ë”ì—ì„œ ì²˜ë¦¬? </summary>
  <div markdown="1">

  Encoder ë‚´ë¶€ë¥¼ ì¢€ ë” ìì„¸í•˜ê²Œ ë³´ë©´ **Self-Attention**ê³¼ **Feed Forward Neural Network**ë¼ëŠ” 2ê°œì˜ Sub layerë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.  

    ![s1](/assets/images/AI-Images/img49.png)

    â—Self-attention : 3ê°œì˜ ë‹¨ì–´ì— ëŒ€í•œ 3ê°œì˜ ë²¡í„° -> 3ê°œì˜ ë²¡í„°ë¥¼ ì¶œë ¥í•´ì¤€ë‹¤.  
    ì—¬ê¸°ì„œ self-attentionì€ ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤ì— ëŒ€í•´ dependenciesê°€ ì¡´ì¬í•œë‹¤.  
    ex) self-attention at high-level  

    â—Feed forward Neural Network : self-attentionì—ì„œ ì¶œë ¥ëœ vectorë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.  

    ![s1](/assets/images/AI-Images/img75.png)

    â—Word Embedding  
    ê° ë‹¨ì–´ë“¤ì„ Inputì— ì´ìš©í•˜ê¸° ìœ„í•´ì„œ Embedding Vectorë¡œ ë§Œë“­ë‹ˆë‹¤.  
    [Embedding Algorithms ê´€ë ¨ ë§í¬](https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca)

    ![s1](/assets/images/AI-Images/img74.png)

  - Encoding ê³¼ì •  

    <details>
    <summary>Self-Attention</summary>
    <div markdown="1">

    "Thinking Machines"ë¼ëŠ” Sentenceê°€ Encoding ë˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

    ![s1](/assets/images/AI-Images/img76.png)

    1. Self-Attentionì—ì„œ í•´ì•¼í•  ì²« ë‹¨ê³„ëŠ” 3ê°œì˜ ë²¡í„°ë¥¼ ë§Œë“œëŠ” ì¼ì…ë‹ˆë‹¤. ê° 3ê°œì˜ í•™ìŠµ ê°€ëŠ¥í•œ í–‰ë ¬ë“¤ì„ ê³±í•´ì„œ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤.  

        ì—¬ê¸°ì„œ ì•Œì•„ë‘ì–´ì•¼í•  ì ì€, ê¸°ì¡´ ì…ë ¥ ë²¡í„°ë“¤ì˜ í¬ê¸°ê°€ 512(ê·¸ë¦¼ì—ì„œëŠ” 4)ì¸ ë°˜ë©´ ìƒˆë¡œìš´ ë²¡í„°ë“¤ì˜ í¬ê¸°ëŠ” 64 (ê·¸ë¦¼ì—ì„œëŠ” 3) ì´ ë©ë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ” í›„ì— Multi-head Attentionì˜ ê³„ì‚° ë³µì¡ë„ë¥¼ ë§ì¶”ê¸° ìœ„í•´ì„œ headì˜ ê°œìˆ˜ë§Œí¼ dimensionì„ ë‚˜ëˆ ì¤¬ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.  
        ğŸ’¡64 = 512 / 8 (head)  

        ![s1](/assets/images/AI-Images/img78.png){: width="80%" height="80%"}

        <br>

    2. Embedding vector / Queries / Keys / Values ì˜ ë²¡í„°ë“¤ì„ í•˜ë‚˜ì”© ìƒì„±í•´ì£¼ê³  Queries, Keys vectorì‚¬ì´ì˜ ë‚´ì ì„ í†µí•´ Score vectorë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤. (ì–¼ë§ˆë‚˜ ê´€ê³„ ìˆëŠ”ì§€)  
        ğŸ’¡ë‚´ì ì„ í•  ë–„ëŠ” í˜„ì¬ ë‹¨ì–´ì™€ ì´ì „, ì´í›„ keyì™€ì˜ ë‚´ì ì„ í†µí•´ì„œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤. 
        ğŸ’¡ì—¬ê¸°ì„œ ë§í•˜ëŠ” Queries, keys, valuesëŠ” ì¶”ìƒì ì¸ ê°œë…!  

        ![s2](/assets/images/AI-Images/img50.png)

        <br>

    3. ê·¸ ë‹¤ìŒ Scoreë¥¼ Normalization (Key Vectorì˜ dimensionì˜ ì œê³±ê·¼) í•œ í›„, Softmax í•¨ìˆ˜ë¥¼ í†µí•´ ê³„ì‚°í•œ ë‹¤ìŒì— ê°€ì§€ê³  ìˆëŠ” Value vectorì— ê³±í•´ì¤€ë‹¤. (weighted Sum of the value vectors)  
      ğŸ’¡Normalizationì˜ ì´ìœ ëŠ” ê° Wì˜ dimensionì´ ë†’ì•„ì§ì— ë”°ë¼ ê° ìš”ì†Œë³„ë¡œ í¬ê¸°ê°€ ì»¤ì§€ê¸°ì— ë¶ˆì•ˆì •í•œ Gradientë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

        R-CNN - LSTMì—ì„œì˜ ì—­í•   
        Q : ë””ì½”ë”ì˜ ì´ì „ ë ˆì´ì–´ hidden state  
        K : ì¸ì½”ë”ì˜ output state  
        V : ì¸ì½”ë”ì˜ output state  

        ![s3](/assets/images/AI-Images/img51.png)

      ì´ êµ¬ì¡°ëŠ” ì…ë ¥ê³¼ ì¶œë ¥ì´ ê³ ì •ë˜ì–´ ìˆëŠ” êµ¬ì¡°ê°€ ì•„ë‹ˆë¼, ì…ë ¥ì— ì–´ë–¤ ë‹¤ë¥¸ Sequenceê°€ ë“¤ì–´ì˜¤ë©´ ì¶œë ¥ì´ ê³„ì† ë‹¬ë¼ì§€ê²Œ ëœë‹¤.  
      ì¦‰, í‘œí˜„ë ¥ì´ ë§ì•„ì§„ë‹¤. -> ë§ì€ computation(ê³„ì‚°) ì´ í•„ìš”  

      ğŸ’¡Key, Query, Value vector ê°ê° ì°¾ì•„ë‚´ëŠ” MLPê°€ ì¡´ì¬í•œë‹¤. 

      <br>

      *ìœ„ì˜ ê³¼ì •ì„ í–‰ë ¬ ë‹¨ìœ„ë¡œ, ë‹¤ì‹œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.*  

      ![s1](/assets/images/AI-Images/img77.png){: width="80%" height="80%"}

      ![s4](/assets/images/AI-Images/img52.png)

      ì§€ê¸ˆê¹Œì§€ì˜ ëª¨ë“  ê³¼ì •ì„ í•˜ë‚˜ì˜ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´, ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  
      ì—¬ê¸°ì„œ ì•Œì•„ì•¼í•  ì ì€ ì²«ë²ˆì§¸ Attentionì—ì„œ Embedding ê³¼ì •ì´ ìˆê³  ë‚˜ë¨¸ì§€ëŠ” Encoderë¥¼ í†µí•´ ë‚˜ì˜¨ z vectorë¥¼ ê·¸ëŒ€ë¡œ Inputìœ¼ë¡œ í•©ë‹ˆë‹¤.  
      z vectorì˜ sizeëŠ” Encoder ê³¼ì •ì„ ê±°ì¹œ í›„, Inputì˜ sizeì™€ ê°™ê²Œ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤.  

      ![s4](/assets/images/AI-Images/img86.png)

    </div>
    </details>

    <details>
    <summary>Multi Headed Attention (MHA)</summary>
    <div markdown="1">

    í•˜ë‚˜ì˜ Inputì— ìœ„ì™€ ê°™ì€ Self-attentionì„ ë°˜ë³µí•´ì„œ Nê°œì˜ Headë¥¼ ë§Œë“¤ê²Œ ë˜ëŠ” ë°, ì´ë¥¼ MHAë¼ê³  í•œë‹¤.  
    MHAë¥¼ í•˜ëŠ” ì´ìœ ëŠ”  
    ì²«ë²ˆì§¸ë¡œ, ëª¨ë¸ì´ ë‹¤ë¥¸ ìœ„ì¹˜ì—ì„œë„ ì§‘ì¤‘í•˜ëŠ” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì˜ˆì œ) â€œThe animal didnâ€™t cross the street because it was too tiredâ€ì—ì„œ itì´ ê°€ë¦¬í‚¤ëŠ” ê²ƒì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ” ì§€ ì•Œì•„ë‚¼ ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.  
    ë‘ë²ˆì§¸ë¡œ, Attentionì´ ì—¬ëŸ¬ ê°œì˜ representation layerë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤.  

    ![s5](/assets/images/AI-Images/img78.png)

    self-Attention ì´í›„, ì„œë¡œ ë‹¤ë¥¸ 8ê°œì˜ zí–‰ë ¬ì´ ë‚˜ì˜¤ê²Œ ë˜ëŠ”ë°, ì´ í–‰ë ¬ì„ ë°”ë¡œ feed-forward layerë¡œ ì“¸ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ í•œ ìœ„ì¹˜ì— ëŒ€í•´ ì˜¤ì§ í•œ ê°œì˜ í–‰ë ¬ë§Œ Inputìœ¼ë¡œ ë°›ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.  

    ![s5](/assets/images/AI-Images/img79.png)

    ì´ëŸ¬í•œ MHAë¥¼ í†µí•´ ë‚˜ì˜¨ Z ë²¡í„°ë“¤ì€ concatenatingì„ í•˜ê³  W0ë¼ëŠ” í° ê°€ì¤‘ì¹˜ ë²¡í„°ì— ì˜í•´ output Z ìƒì„±í•˜ê²Œ ë©ë‹ˆë‹¤. (CNNì˜ ë§ˆì§€ë§‰ì— flatten, Concatí•˜ëŠ” ê³¼ì •ê³¼ ë¹„ìŠ·)

    ![s5](/assets/images/AI-Images/img53.png)

    Self-Attentionê³¼ MHAë¥¼ í†µê³¼í•œ í›„, itì´ë¼ëŠ” ë‹¨ì–´ë¥¼ encodeí•  ë•Œ, ê°ê°ì˜ Attentionë“¤ì´ ì–´ë–¤ ë‹¨ì–´ë¥¼ ê°€ë¦¬í‚¤ëŠ” ì§€ ë³´ë©´, ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.  
    ì²«ë²ˆì§¸ ê·¸ë¦¼ì˜ ê²½ìš°, itì´ "The", "animal" ê³¼ "tire", "-d"ì— ì§‘ì¤‘í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„, itì˜ representationì— í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
    ë‘ë²ˆì¨° ê·¸ë¦¼ì˜ ê²½ìš°, itì´ ìœ„ì˜ ë‹¨ì–´ë§ê³ ë„ ë§ì€ ë‹¨ì–´ë¥¼ representaitonì— í¬í•¨ì‹œí‚¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

    ![s5](/assets/images/AI-Images/img80.png)

    ![s5](/assets/images/AI-Images/img81.png)

    ë§ì€ ì •ë³´ë¥¼ ë‹´ê³  ìˆì§€ë§Œ, ìœ„ì¹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤. -> Positional Encoding

    </div>
    </details>

    <details>
    <summary>Positional Encoding</summary>
    <div markdown="1">

    Positional Encodingì€ ëª¨ë¸ì—ê²Œ ë‹¨ì–´ì— ëŒ€í•œ ìˆœì„œì˜ ì •ë³´ë¥¼ ì£¼ê¸° ìœ„í•´ì„œ ìœ„ì¹˜ë³„ë¡œ íŠ¹ì • íŒ¨í„´ì„ ë”°ë¥´ëŠ” ë²¡í„°ì…ë‹ˆë‹¤.  

    ![s5](/assets/images/AI-Images/img82.png){: width="80%" height="80%"}

    ì•„ë˜ ê·¸ë¦¼ì€ Embedding í¬ê¸°ê°€ 4ì¼ ë•Œ (ì‹¤ì œë¡œëŠ” 512) ì˜ˆì‹œì…ë‹ˆë‹¤.  
    SequenceëŠ” ì–´ë–»ê²Œ ë“¤ì–´ì˜¤ë“  ê°’ì´ encodingì— ì˜í•´ ë‹¬ë¼ì§ˆ ìˆ˜ê°€ ì—†ë‹¤. (orderì— independent)  
    ì–´ë–¤ ë‹¨ì–´ê°€ ì–´ëŠ ìœ„ì¹˜ì— ìˆëŠ” ì§€ ì¤‘ìš”í•˜ê¸°ì— ë„£ì–´ì¤€ë‹¤. (ë²¡í„°ê°’ì— offsetì„ ì¤Œ)  

    ![s6](/assets/images/AI-Images/img54.png)

    ì‹¤ì œ Encoding Vectorì˜ ì‹œê°í™”  

    ![s5](/assets/images/AI-Images/img83.png){: width="75%" height="75%"}

    </div>
    </details>

    <details>
    <summary>The Residuals</summary>
    <div markdown="1">

    Encoderë¥¼ ë” ëœ¯ì–´ë³´ë©´, ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  
    Self-Attention ì´í›„, Feed Forwardë¥¼ ë°”ë¡œ ì§„í–‰í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ Layer Normalization ê³¼ì •ì„ í•œë²ˆ ê±°ì¹˜ê²Œ ë©ë‹ˆë‹¤.  
    [Layer Normalizationì— ê´€í•œ ë…¼ë¬¸ ë§í¬](https://arxiv.org/abs/1607.06450)  
    Sub layerì—ë„ ì ìš©í•œë‹¤ë©´ ë‘ë²ˆì§¸ ê·¸ë¦¼ê³¼ ê°™ì´ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  

    ![s5](/assets/images/AI-Images/img83.png){: width="80%" height="80%"}

    ![s5](/assets/images/AI-Images/img84.png){: width="80%" height="80%"}


    </div>
    </details>

  </div>
  </details>


  <details>
  <summary> b. ì¸ì½”ë”ì™€ ë””ì½”ë” ì‚¬ì´ì— ì–´ë–¤ ì •ë³´ ì£¼ê³  ë°›ìŒ? </summary>
  <div markdow="1">

  ![s7](/assets/images/AI-Images/img55.png)

  Encoderì—ì„œ Decoderë¡œ ê°ˆ ë•Œ, ì–´ë–¤ ì •ë³´ê°€ ì „ë‹¬ë˜ì–´ì§€ëŠ”ê°€?

  ![s8](/assets/images/AI-Images/img56.png)
  ![s9](/assets/images/AI-Images/img57.png)

  Key Code ì™€ Value vector ê°’ì„ Decoderì— ì „ë‹¬í•´ì„œ previous outputê³¼ í•¨ê»˜ outputì´ ë‚˜ì˜¤ê²Œ ëœë‹¤.

  - Self-attention layer & Encoder-Decoder Attention  
      Masking :  ì´ì „ ë‹¨ì–´ë“¤ì—ë§Œ dependent í•˜ë„ë¡ í•˜ê³ ,.. ë¬´ìŠ¨ ëœ»ì¸ì§€ ì´í•´ ë¶ˆê°€..  

      ![s10](/assets/images/AI-Images/img58.png)
      ![s11](/assets/images/AI-Images/img59.png)

  
  c. ë””ì½”ë”ê°€ ì–´ë–»ê²Œ generalize í•  ìˆ˜ ìˆëŠ”ì§€?  


