---
title: "[네이버 부스트 캠프] AI-Tech-re - week1(3)"
categories: AI
tags: python
published: true
use_math: true
---

## 학습기록 - 3

오늘 한 일

- 딥러닝, 확률론, 통계학론, 베이즈 통계학 ppt
- Gradient Descent 코드 구현


### 1. 새로 알게된 내용

  - 선형 모델, 비선형 모델의 정의

  - 확률론 속 조건부 확률과 기대값, 몬테카를로 샘플링

  - 모수를 통한 통계 모델의 확률 분포 정의 (표본 평균, 표본 분산)
    최대 가능도 추정법 (MLE), 쿨백 라이블러 발산
  
  - 베이즈 확률 - 조건부 확률을 기반으로 하여 확률을 갱신시켜  
                 검사율을 만드는 확률 모델 -> False Alarm 과 Prescision의 관계
               - 조건부 확률을 쓸 때 주의할 인과 관계

### 2. 고민 내용, 결과

  Gradient Descent 문제  

  1. 정의에 대한 이해를 다 했다고 생각했음에도 코드 구현 시 어려움을 겪었다.  

  2. `다음 기울기 = 현재 기울기 - lr * 현재 기울기` 이를 기본으로 적절한 기울기를 찾을 때까지 x값을 움직였어야 했는데, 아예 생각조차 하지 못했다.  

  3. Linear Regression에서 Gradient_W 와 Gradient_b에 대한 정의를 알고 있음에도 코드로 제대로 생각하지 못했다. Gradient 자체는 편미분 값을 의미한다.

  ```python
  gradient_w = - 2 * np.mean((train_y - (_y)) * (train_x))
  gradient_b = - 2 * np.mean((train_y - (_y)))
  ```

  4. numpy 연산자

  - *\ asterisk operator : 형태가 동일한 두 행렬의 원소 곱을 의미

  - dot : 내적곱, 첫번째 행렬의 열크기와 두번째 행렬의 행크기가 같아야함

  - matmul(@) : 행렬곱, 내적곱과 행렬에서의 결과는 같지만 matmul은 스칼라 값을 하지는 못한다

  5. SGD

  mini-batch를 짤 때 y값의 원소를 넣어주지 않고, 식을 넣어서 틀림. 오류를 제대로 찾지 못해 헤맸다.  
  다행히 찾게 되어 해결
  error 값 : 모두 L2 norm으로 error을 나타낼 수 있을 줄 알았는 데 sgd에서는 오류가 걸린다.  
  error는 해결하지 못했다.  

### 4. 회고

  임성빈 교수님의 말씀을 통해 다시 의지를 태우게 되었던 것 같다.  

  요약하자면,  

  1. 엔지니어가 되기 위해서 필요 기초 지식인 __선형 대수, 확률론, 통계학__ 을 제대로 알고,  
  2. 지식을 알아도 **코드 베이스 활용 공부** 필수
  3. 수학적 지식이 어렵다면, 용어의 정의를 일단 외우고 익숙해질 때까지 빈도 수를 늘리기  
  4. 그리고 가장 중요한, 엔지니어로써 **빠르게 문제를 확립하고 솔루션으로 연결** 하는 버릇을 기르기
  5. 인공지능 대학원은 새로운 분야에 대해 원하면!
  6. 전문성을 가진 사람 - 논문 구현(codeless), 전문성 평가, 교수님 추천(?)
  7. 질문하자

  책, Dive into Deeplearning / 밑바닥부터 시작하는 딥러닝 / Book of why (역설에 대한 설명)

  어제, 오늘은 후회되는 하루를 보낸 것 같다.  
  짧게 요약해도 될 것을 길게 늘리고, 보기 귀찮다고 대충 흘겨 봐서 제대로 이해하지 못해 4번 넘게 보게되어 시간이 낭비되었다.  
  
  시간을 효율적으로 써야하는 데 잘 안되는 기분이다.  
